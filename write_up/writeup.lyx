#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass scrartcl
\begin_preamble
% from the icml 2016 example tex file
\usepackage{times}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2016} 

%\usepackage[accepted]{icml2016}


\newcommand{\heiko}[1]{   {\bf \color{blue}{HS: #1}}  }
\newcommand{\kacper}[1]{   {\bf \color{red}{K: #1}}  }
\newcommand{\arthur}[1]{   {\bf \color{magenta}{AG: #1}}  }

%\newcommand{\heiko}[1]{}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 12
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 2
\use_esint 0
\use_mhchem 1
\use_mathdots 1
\cite_engine natbib_numerical
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[ 
\backslash
icmltitle{A Kernel Test of Goodness of Fit}
\end_layout

\begin_layout Plain Layout

% It is OKAY to include author information, even for blind
\end_layout

\begin_layout Plain Layout

% submissions: the style file will automatically remove it for you
\end_layout

\begin_layout Plain Layout

% unless you've provided the [accepted] option to the icml2015
\end_layout

\begin_layout Plain Layout

% package.
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Kacper Chwialkowski$^*$}{kacper.chwialkowski@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Heiko Strathmann$^*$}{heiko.strathmann@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Arthur Gretton}{arthur.gretton@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Gatsby Unit, University College London, United Kingdom}
\end_layout

\begin_layout Plain Layout

% You may provide any keywords that you 
\end_layout

\begin_layout Plain Layout

% find helpful for describing your paper; these are used to populate
\end_layout

\begin_layout Plain Layout

% the "keywords" metadata in the PDF but will not be shown in the document
\end_layout

\begin_layout Plain Layout


\backslash
icmlkeywords{kernel methods, goodness-of-fit, Stein's method, statistical
 testing}
\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Abstract
We propose a nonparametric statistical test for goodness-of-fit: given a
 set of samples, the test determines how likely it is that these were generated
 from a target probability density function.
 The measure of goodness-of-fit is an integral probability metric constructed
 from a Reproducing Kernel Hilbert Space via Stein's method.
 Our test statistic is an unbiased empirical estimate of this measure, taking
 the form of a simple V-statistic in terms of the log gradients of the target
 and the kernel.
 We derive a statistical test, both for i.i.d.
 and non-i.i.d.
 samples, where in the latter setting we propose a wild bootstrap procedure.
 We apply our test to quantifying convergence of approximate Markov Chain
 Monte Carlo methods, statistical model critisism, and evaluating quality
 of fit vs model complexity in nonparametric density estimation.
\end_layout

\begin_layout Standard

\lang english
\begin_inset FormulaMacro
\newcommand{\ev}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Goodness-of-fit testing, or measuring sampling quality, is a fundamental
 tool in statistical analysis, dating back to the test of Kolmogorov and
 Smirnov 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolmogorov33,Smirnov48"

\end_inset

.
 Given a set of samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

 with distribution 
\begin_inset Formula $Z_{i}\sim q$
\end_inset

, our interest is in whether 
\begin_inset Formula $q$
\end_inset

 matches some reference or target distribution 
\begin_inset Formula $p$
\end_inset


\lang english
, which we assume to be only known up to the normalisation constant.
\end_layout

\begin_layout Standard

\lang english
Our measure of goodness of fit builds on recent work of 
\lang british

\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

, who proposed an elegant measure of sample quality with respect to a target.
 This is a maximum discrepancy between empirical sample expectations and
 target expectations over a large class of test functions, constructed so
 as to have zero expectation over the target distribution by use of a Stein
 operator.
 This operator depends only on the derivarive of the 
\begin_inset Formula $\log q$
\end_inset

: thus, the approach can be applied very generally, as it does not require
 closed-form integrals over the target distribution (or numerical approximations
 of such integrals).
 By contrast, many earlier discrepancy measures require integrals with resepect
 to the target (see below for a review).
 This is problematic if the intention is to perform benchmarks for assessing
 Markov Chain Monte Carlo, since these integrals will certainly not be known
 to the practitioner.
\end_layout

\begin_layout Standard
A challenge in applying the approach of 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

 is the complexity of the function class used, which results from applying
 the Stein operator to the bounded Lipschitz functions (which give rise
 to the Wasserstein integral probability metric).
 Thus, their sample quality measure requires solving an expensive linear
 program that arises from a complicated construction of graph Stein discrepancie
s and geometric spanners.
 Their metric furthermore requires access to nontrivial lower bounds that,
 despite being provided for log-concave densities, are a largely open problem
 otherwise, in particular for multivariate cases.
\end_layout

\begin_layout Standard
An important application of a goodness-of-fit measure is in statistical
 testing, where it is desired to determine whether the measure is large
 enough to reject the null hypothesis (that the sample arises from the target
 distribution).
 One approach is to establish the asymptotic behaviour of the test statistic,
 and to set a test threshold at a large quantile of he asymptotic distribution.
 The asymptotic behaviour of the Wasserstein-based Stein discrepancy remains
 a challenging open problem due to the complexity of the function class
 used.
 Thus, it is not clear how one would compute p-values for this statistic,
 or determine when the goodness of fit allows one to accept the null hypothesis
 (at the user-specified test level).
\end_layout

\begin_layout Standard
The key contribution of this work is to define a statistical test of goodness-of
-fit, based on a Stein discrepancy computed in a Reproducing Kernel Hilbert
 Space (RKHS).
 To construct our test statistic, we apply the Stein operator to our chosen
 RKHS functions, and define our measure of goodness of fit as the largest
 discrepancy between empirical sample expectations and target expectations
 over the resulting test functions.
 This approach is a natural extension to goodness-of-fit testing of the
 earlier two-sample tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton2012kernel"

\end_inset

 and independence tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton_kernel_2008"

\end_inset

 based on the maximum mean discrepancy, which is an integral probability
 metric.
 As with these earlier tests, our statistic is a simple U-statistic, and
 can be computed in closed form and in quadratic time; moreover, it is an
 unbiased estimate of the corresponding population discrepancy.
 As with all Stein-based discrepancies, only the gradient of the log-density
 of the target density is needed; we do not require integrals with respect
 to the target density.
 Given that our test statistic is a U-statistic, we may make use of the
 extensive literature on asymptotics of U-statistics to formulate a hypothesis
 test 
\begin_inset CommandInset citation
LatexCommand citep
key "serfling80,leucht_dependent_2013"

\end_inset

.
 We are able to provide statistical tests even in the case of correlated
 samples, which is essential if the test is to be used in assessing the
 quality of output of an MCMC procedure.
\end_layout

\begin_layout Standard
Several alternative approaches exist in the statistics literature to perform
 goodness-of-fit testing.
 A first strategy is to partition the space, and to conduct the test on
 a histogram estimate of the distribution 
\begin_inset CommandInset citation
LatexCommand citep
key "Bar89,Beirlant2,Gyorfi,GyVa02"

\end_inset

.

\lang english
 Such space parititioning approaches can have attractive theoretical properties
 (eg distribution-free test thresholds) and work well in low dimensions,
 however they are much less powerful than alternatives once the dimensionality
 increases 
\begin_inset CommandInset citation
LatexCommand cite
key "GreGyo10"

\end_inset

.

\lang british
 A second popular approach has been to use the smoothed 
\begin_inset Formula $L_{2}$
\end_inset

 distance between the empirical chacacteristic function of the sample, and
 the characteristic function of the target density.
 This dates back to the test of Gaussianity of 
\begin_inset CommandInset citation
LatexCommand citet
key "BaringhausHenze88"

\end_inset

, who used a squared exponential smoothing function (see Eq.
 2.1 in their paper).
 For this choice of smoothing function, their statistic is identical to
 the maximum mean discrepancy (MMD) with the squared exponential kernel,
 which can be shown using the Bochner representation of the kernel (compare
 with 
\begin_inset CommandInset citation
LatexCommand citealt
after "Corollary 4"
key "SriGreFukLanetal10"

\end_inset

).
 It is essential in this case that the target distribution be Gaussian,
 since the convolution with the kernel (or in the Fourier domain, the smoothing
 function) must be available in closed form.
 An 
\begin_inset Formula $L_{2}$
\end_inset

 distance between Parzen window estimates can also be used  
\begin_inset CommandInset citation
LatexCommand citep
key "BowFos93"

\end_inset

, giving the same expression again, although the optimal choice of bandwidth
 for consistent Parzen window estimates may not be a good choice for testing
 
\begin_inset CommandInset citation
LatexCommand citep
key "AndHalTit94"

\end_inset

.
 A different smoothing scheme in the frequency domain results in an energy
 distance statistic (this likewise being an MMD with a particular choice
 of kernel 
\begin_inset CommandInset citation
LatexCommand citep
key "SejSriGreFuk13"

\end_inset

), which can be used in a test of normality 
\begin_inset CommandInset citation
LatexCommand citep
key "SzeRiz05"

\end_inset

.
 The key point is that the required integrals are again computable in closed
 form for the Gaussian (this may be extended to certain other families of
 interest, e.g.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Rizzo09"

\end_inset

).
 The requirement of computing closed-form integrals with respect to the
 test distribution severely restricts this testing strategy.
\end_layout

\begin_layout Standard
A specific application of this goodness-of-fit test is that it is able to
 correctly handle samples from approximate Markov Chain Monte Carlo (MCMC)
 
\begin_inset CommandInset citation
LatexCommand citep
key "Korattikara2014,Welling2011,Bardenet2014"

\end_inset

.
 With the hope to increase the overall efficiency, these methods use modificatio
ns to Markov transition kernels that improve mixing speed at the cost of
 introducing an asymptotic bias.
 Such bias-variance trade-offs can usually be tuned with parameters of the
 sampling algorithms.
 It is therefore an important question whether for a particular parameter
 setting and for a fixed run-time, the produced samples have increased in
 quality or not.
 This question cannot be answered with classical MCMC convergence statistics
 alone, such as Effective Sample Size, Gelman-Rubin -- those assume that
 the Markov chain reaches its equilibrium distribution.
 The described goodness-of-fit testing framework, however, fits very well
 into this context as it exactly quantifies the asymptotic bias of approximate
 MCMC.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
We begin our presentation in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-Kernel-Goodness-of-fit"

\end_inset

 with a high-level construction of the test.
 In the 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

 we prove the main results presented in the previous section.
 In the section 
\end_layout

\end_inset


\end_layout

\begin_layout Section

\lang english
A Kernel Goodness-of-fit Test
\begin_inset CommandInset label
LatexCommand label
name "sec:A-Kernel-Goodness-of-fit"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We now give a high-level construction of our divergence metric and statistical
 test construction.
 While this section aims to communicate the main ideas, and is restricted
 to the real-valued case, we provide the multivariate counterpart, details,
 and proofs in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

.
\end_layout

\begin_layout Subsection

\lang english
Stein Operator in RKHS
\end_layout

\begin_layout Standard

\lang english
Our goal is to write the maximum discrepancy between reference distribution
 
\begin_inset Formula $p$
\end_inset

 and observed sample distribution 
\begin_inset Formula $q$
\end_inset

 in a RKHS.
 Denote by 
\begin_inset Formula ${\cal F}$
\end_inset

 the RKHS of real-valued functions with reproducing kernel 
\begin_inset Formula $k$
\end_inset

, and by 
\begin_inset Formula ${\cal F}^{d}$
\end_inset

 the product RKHS consisting of elements 
\begin_inset Formula $f:=(f_{1},\dots,f_{d})$
\end_inset

 with 
\begin_inset Formula $f_{i}\in{\cal F}$
\end_inset

, and with standard inner product.
 Similarly to 
\begin_inset CommandInset citation
LatexCommand citet
key "stein1972,gorham2015measuring"

\end_inset

, we begin by defining a Stein operator 
\begin_inset Formula $T$
\end_inset

 acting on 
\begin_inset Formula $f\in\mathcal{F}^{d}$
\end_inset

 
\begin_inset Formula 
\[
Tf:=\sum_{i=1}^{d}\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}.
\]

\end_inset

As we will see, the operator can be expressed in the product RKHS by defining
 a function that depends on gradients of the log-density and the kernel
 
\begin_inset Formula 
\[
\xi(x,\cdot):=\left[\nabla\log p(x)k(x,\cdot)+\nabla k(x,\cdot)\right],
\]

\end_inset

whose inner product with 
\begin_inset Formula $f$
\end_inset

 exactly gives the expected value of the Stein operator, for our reference
 
\begin_inset Formula $X\sim p$
\end_inset

 
\begin_inset Formula 
\[
\ev Tf(X)=\langle f,\ev\xi(X)\rangle_{{\cal F}^{d}}=\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(X)\rangle_{{\cal F}},
\]

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

.
 The operator produces mean-zero functions: 
\begin_inset Formula $\langle f,\ev\xi(X)\rangle=0$
\end_inset

, which can be seen by integration by parts, c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

.
 We can now define a Stein discrepancy between 
\begin_inset Formula $X\sim p$
\end_inset

 and the observed 
\begin_inset Formula $Z\sim q$
\end_inset

 -- and express it in the RKHS,
\begin_inset Formula 
\begin{align*}
S(Z) & :=\sup_{\Vert f\Vert<1}\ev(Tf)(Z)-\ev(Tf)(X)\\
 & =\sup_{\Vert f\Vert<1}\langle f,\ev\xi(Z)-\ev\xi(X)\rangle\\
 & =\|\ev\xi(Z)\|,
\end{align*}

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:discprepancy_maximised_by_norm"

\end_inset

.
 This makes it clear why 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

 is a desirable property: we can compute 
\begin_inset Formula $S(Z,p)$
\end_inset

 by only computing 
\begin_inset Formula $\|\ev\xi(Z)\|$
\end_inset

 -- without the need to access 
\begin_inset Formula $X$
\end_inset

 in the form of samples from 
\begin_inset Formula $p$
\end_inset

.
 We arrive at our first main result, which states that the above discrepancy
 can be used to distinguish two distributions 
\begin_inset Formula $p,q\in{\cal P}$
\end_inset

 .
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "theorem_discrepancy_is_metric"

\end_inset

 Let 
\begin_inset Formula $q,p\in\mathcal{P}$
\end_inset

 and let 
\begin_inset Formula $Z\sim q$
\end_inset

.
 Then 
\begin_inset Formula $S(Z)=0$
\end_inset

 if and only if 
\begin_inset Formula $p=q$
\end_inset

.
 
\end_layout

\begin_layout Standard
Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

 contains details such as all assumptions on 
\begin_inset Formula ${\cal P}$
\end_inset

 and proof.
 The following theorem gives a simple closed form expression.
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "th:closed_form_discrepancy"

\end_inset

 Let 
\begin_inset Formula 
\begin{align*}
h(x,y) & :=\nabla\log p(x)^{\top}\nabla\log p(y)k(x,y)\\
 & \quad+\nabla\log p(y)^{\top}\nabla_{x}k(x,y)\\
 & \quad+\nabla\log p(x){}^{\top}\nabla_{y}k(x,y)\\
 & \quad+\nabla_{x}k(x,y)^{\top}\nabla_{y}k(x,y).
\end{align*}

\end_inset

The 
\lang british
squared Stein discrepancy has
\lang english
 the closed form 
\begin_inset Formula $S(Z)^{2}=\ev h(Z,Z')$
\end_inset

.
\end_layout

\begin_layout Standard
We now proceed with constructing an estimator for 
\begin_inset Formula $S(Z)^{2}$
\end_inset

, and outline its asymptotic properties.
\end_layout

\begin_layout Subsection
Wild Bootstrap Testing
\end_layout

\begin_layout Standard
It is straight forward to estimate the squared Stein discrepancy 
\begin_inset Formula $S(Z)^{2}$
\end_inset

 from samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

: a quadratic time estimator is an average of pairwise terms, so called
 V-Statistic, which takes the form
\begin_inset Formula 
\[
V_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}h(Z_{i},Z_{j}).
\]

\end_inset

 The asymptotic null distribution of the normalised V-Statistic 
\begin_inset Formula $nV_{n}$
\end_inset

, however, has no computable closed form.
 Furthermore, care has to be taken when the 
\begin_inset Formula $Z_{i}$
\end_inset

 exhibit correlation structure, as the null distribution significantly changes,
 impacting test significance.
 The wild-bootstrap technique 
\begin_inset CommandInset citation
LatexCommand citep
key "Shao2010,leucht_dependent_2013"

\end_inset

 addresses both problems.
 First, it allows to simulate from the null distribution to compute test
 thresholds.
 Second, it accounts for correlation structure in the 
\begin_inset Formula $Z_{i}$
\end_inset

 by mimicing it with a 
\lang english
auxiliary
\lang british
 random process, a simple
\lang english
 Markov chain taking values in 
\begin_inset Formula $\{-1,1\}$
\end_inset

, starting from 
\begin_inset Formula $W_{1,n}=1$
\end_inset

:
\lang british

\begin_inset Formula 
\[
W_{t,n}=\mathbf{1}(U_{t}>a_{n})W_{t-1,n}-\mathbf{1}(U_{t}<a_{n})W_{t-1,n},
\]

\end_inset


\lang english
where the 
\begin_inset Formula $U_{t}$
\end_inset

 are uniform i.i.d.
 random variables and 
\begin_inset Formula $a_{n}$
\end_inset

 is the probability of 
\begin_inset Formula $W_{t,n}$
\end_inset

 changing sign.
 This leads to a bootstrapped V-statistic 
\end_layout

\begin_layout Standard

\emph on
\lang english
\begin_inset Formula 
\[
B_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}W_{i,n}W_{j,n}h(Z_{i,}Z_{j}).
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 establishes that 
\begin_inset Formula $B_{n}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
 is a good approximation
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
of 
\begin_inset Formula $V_{n}$
\end_inset

, so it is possible to approximate quantiles of the null distribution by
 samplig from it.
 We propose the following test
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 procedure:
\end_layout

\begin_layout Itemize
Calculate 
\lang english
the test statistic 
\begin_inset Formula $V_{n}$
\end_inset

.
\end_layout

\begin_layout Itemize

\lang english
Calculate
\lang british
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\{B_{n}\}_{i=1}^{D}$
\end_inset

 
\begin_inset Formula $D$
\end_inset

 times and estimate the 
\begin_inset Formula $1-\alpha$
\end_inset

 empirical quantile 
\begin_inset Formula $q$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
If 
\begin_inset Formula $V_{n}>q$
\end_inset

, reject hypothesis that 
\begin_inset Formula $Z\sim p$
\end_inset

.
\end_layout

\begin_layout Section

\lang english
Details
\begin_inset CommandInset label
LatexCommand label
name "sec:Details"

\end_inset


\end_layout

\begin_layout Subsection

\lang english
Kernel-Stein
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sec:details_kernel_stein"

\end_inset


\end_layout

\begin_layout Standard

\lang english
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
heiko{I just copied the old Section 2 here.
 This now needs thinning having the new Section 2 in mind.}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
The test is applicable to family of distributions, on a real coordinate
 space, 
\begin_inset Formula $\mathcal{P}$
\end_inset

, where distributions 
\begin_inset Formula $p\in\mathcal{P}$
\end_inset

 satisfy two conditions: 
\begin_inset Newline newline
\end_inset

 (i) 
\begin_inset Formula $\nabla\log p(x)$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
is Lipschitz continuous
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
; 
\begin_inset Newline newline
\end_inset

 (ii) 
\begin_inset Formula $\ev\|\nabla\log p(Z)\|^{2}\leq\infty$
\end_inset

 for any random variable 
\begin_inset Formula $Z$
\end_inset

.
\begin_inset Newline newline
\end_inset

 The kernels considered in this work are assumed to be bounded, symmetric
 and cc-universal 
\begin_inset CommandInset citation
LatexCommand citep
key "SriFukLan11"

\end_inset

.
 On top of that we assume that any kernel 
\begin_inset Formula $k$
\end_inset

 must satisfy 
\begin_inset Newline newline
\end_inset

 (iii) 
\begin_inset Formula $\ev\left(\frac{\partial^{2}k(Z,Z)}{dx_{i}dx_{i+d}}\right)^{2}<\infty$
\end_inset

, for any random variable 
\begin_inset Formula $Z$
\end_inset

,
\begin_inset Newline newline
\end_inset

(iv)
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\nabla k(x,y)$
\end_inset

 is Lipschitz continuous.
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Newline newline
\end_inset

We denote by 
\begin_inset Formula $\mathcal{F}$
\end_inset

 the Reproducing Kernel Hilbert Space of scalar-real-valued functions associated
 with the kernel 
\begin_inset Formula $k$
\end_inset

.
 Furthermore 
\begin_inset Formula ${\cal F}^{d}$
\end_inset

 is a product space 
\begin_inset Formula $\otimes_{i=1}^{d}{\cal F}$
\end_inset

 with a standard inner product.
\end_layout

\begin_layout Paragraph

\lang english
Stein operator.
\end_layout

\begin_layout Standard

\lang english
We proceed similarly to 
\begin_inset CommandInset citation
LatexCommand citet
key "stein1972,gorham2015measuring"

\end_inset

 and make use of a Stein operator to characterize discrepancy between probabilit
y measures.
 Following 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

, we study the operator 
\begin_inset Formula $T$
\end_inset

 acting on 
\begin_inset Formula $R^{d}$
\end_inset

 valued functions 
\begin_inset Formula $f:=(f_{1},\cdots,f_{d})$
\end_inset

, 
\begin_inset Formula $f_{i}\in\mathcal{F}$
\end_inset

 
\begin_inset Formula 
\[
Tf=\sum_{i=1}^{d}\left(\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right).
\]

\end_inset

As in 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

, we show that for all 
\begin_inset Formula $f\in{\cal F}^{d},\ev(T_{q}f)(X)=0$
\end_inset

 (Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

).
 Next, for any random variable 
\begin_inset Formula $Z$
\end_inset

, we define Stein discrepancy between 
\begin_inset Formula $X\sim p$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

, 
\begin_inset Formula 
\begin{align}
S(Z) & :=\sup_{f\in F^{d},\|f\|<1}\ev(Tf)(Z)\label{eq:Sdef}\\
 & =\sup_{f\in F^{d}}\ev(Tf)(Z)-\ev(Tf)(X)\nonumber 
\end{align}

\end_inset

 .
 In Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset

 we show that 
\begin_inset Formula $S(Z)$
\end_inset

 captures any difference between probability measures.
 Contrary to 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

, we don't need to approximate 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 -- we can calculate it explicitly, as demonstrated in the Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset

.
 We start with a definition of a function 
\begin_inset Formula $\xi$
\end_inset

, which expected value can be interpreted as a type of a mean embedding.
 
\end_layout

\begin_layout Definition

\lang english
\begin_inset CommandInset label
LatexCommand label
name "def:xi"

\end_inset

For any 
\begin_inset Formula $x\in R^{d}$
\end_inset

, define a vector valued function function 
\begin_inset Formula $\xi:R^{d}\to R^{d}$
\end_inset

, 
\begin_inset Formula 
\[
\xi(x,t)=\left[\nabla\log p(x)k(x,t)+\nabla_{1}k(x,t)\right]
\]

\end_inset

where 
\begin_inset Formula $\nabla\log p(x)=\left(\frac{\partial\log p(x)}{\partial x_{1}},\cdots,\frac{\partial\log p(x)}{\partial x_{d}}\right)$
\end_inset

 and 
\begin_inset Formula $\nabla_{1}k(x,t)=\left(\frac{\partial k(x,t)}{\partial x_{1}},\cdots,\frac{\partial k(x,t)}{\partial x_{d}}\right)$
\end_inset

.
 
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:WellDefined"

\end_inset

 
\begin_inset Formula $\xi(x,\cdot)$
\end_inset

 is an element of the reproducing kernel Hilbert space 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

.
 
\end_layout

\begin_layout Proof

\lang english
We use the proof on p.
 132 of 
\begin_inset CommandInset citation
LatexCommand citet
after "Corollary 4.36"
key "SteChr08"

\end_inset

 to see that for all 
\begin_inset Formula $x\in R^{d}$
\end_inset

 each entry of 
\begin_inset Formula $\nabla_{1}k(x,\cdot)$
\end_inset

 belongs to 
\begin_inset Formula $\mathcal{F}$
\end_inset

.
 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}k(x,\cdot)\in\mathcal{F}$
\end_inset

, since 
\begin_inset Formula $k(x,\cdot)\in\mathcal{F}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}$
\end_inset

 is a scalar.
 
\end_layout

\begin_layout Standard
The following technical lemma shows that the expected value of 
\begin_inset Formula $\xi$
\end_inset

 is well defined -- it is needed for establishing a link between Stain operator
 
\begin_inset Formula $Tf$
\end_inset

 and 
\begin_inset Formula $\xi$
\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:BochnerInt1"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

 is is element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

 (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\xi$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is Bochner integrable wrt measure of 
\begin_inset Formula $Z$
\end_inset

).
 
\end_layout

\begin_layout Standard
Now we can show that the expected value of the Stain operator is a functional
 on the RKHS 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

, hence it coincidences with a inner product with some element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

.
 This element turns out to be the expected value of 
\begin_inset Formula $\xi$
\end_inset

.
 
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:SteinIsInner"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, expected value of Stein operator coincides with inner product of 
\begin_inset Formula $f$
\end_inset

 and expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

 i.e.
 
\begin_inset Formula 
\begin{align*}
\ev Tf(Z)=\langle f,\ev\xi(Z)\rangle_{\mathcal{F}^{d}} & =\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(Z)\rangle_{\mathcal{F}}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof

\lang english
We write
\begin_inset Formula 
\begin{align*}
 & \left\langle f_{i},\ev\xi_{i}(Z)\right\rangle _{\mathcal{F}}\\
 & =\left\langle f_{i},\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right]\right\rangle _{\mathcal{F}}\\
 & =\ev\left\langle f_{i},\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right\rangle _{\mathcal{F}}\\
 & =\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}f_{i}(Z)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right].
\end{align*}

\end_inset

The second equality follows form the fact that linear operator 
\begin_inset Formula $\langle f_{i},\cdot\rangle_{\mathcal{F}}$
\end_inset

 can be interchanged with Bochner integral and the fact that 
\begin_inset Formula $\xi$
\end_inset

 is Bochner integrable (Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt1"

\end_inset

).
 The last equality is an application of reproducing property.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
By the construction of the Stein operator we know that for 
\begin_inset Formula $X\sim p$
\end_inset

, 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

, and so for any random variable 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard

\lang english
\begin_inset Formula 
\begin{align*}
S(Y,\mathcal{F},p) & =\sup_{f\in F^{d}}\ev(Tf)(Y)-\ev(Tf)(X)\\
 & =\sup_{f\in F^{d}}\langle f,\ev\xi(Y)-\ev\xi(X)\rangle_{\mathcal{F}^{d}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
This illustrates intuition 
\begin_inset Formula $\ev\xi(Z)$
\end_inset

 is a type of mean embedding centred at the measure 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:discprepancy_maximised_by_norm"

\end_inset

Discrepancy 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximized by an expected value of 
\begin_inset Formula $\xi$
\end_inset

, 
\begin_inset Formula $S(Y,\mathcal{F},p)=\|\ev\xi(Y)\|$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
By the Lemma 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\ev Tf(Y)=\langle f,\ev\xi(Y)\rangle$
\end_inset

, therefore 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximized by 
\begin_inset Formula $\frac{\ev\xi(Y)}{\|\ev\xi(Y)\|}$
\end_inset

.
\end_layout

\begin_layout Standard
We are ready to write down the closed form formula for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $S(Y,\mathcal{F},p)^{2}$
\end_inset

.
 The proof, which is mostly an algebraic manipulation, is in the appendix.
 
\end_layout

\begin_layout Standard
Finally we prove that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $S$
\end_inset

 discriminates different probability measures.
 
\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument
status open

\begin_layout Plain Layout

\lang english
Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset


\end_layout

\end_inset

If 
\begin_inset Formula $p=q$
\end_inset

 then 
\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

 by the Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

.
 Suppose 
\begin_inset Formula $p\neq q$
\end_inset

, but 
\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

.
 If 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

 then 
\begin_inset Formula $\ev\xi(Y)=0.$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 For each dimension of 
\begin_inset Formula $\ev\xi(Y)$
\end_inset

 we add and subtract 
\begin_inset Formula $\log q(Y)$
\end_inset

 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial}{\partial x_{i}}\log p(Y)k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & =\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & \quad+\ev\left(\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)\right).
\end{align*}

\end_inset

We have used Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

 to see that 
\begin_inset Formula 
\[
\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)=0.
\]

\end_inset

We recognize that expected value of random 
\begin_inset Formula $\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)$
\end_inset

 is mean emending of a function 
\begin_inset Formula $g(y)=\frac{\partial}{\partial x_{i}}\left(\log\frac{p(y)}{q(y)}\right)$
\end_inset

 with respect to the measure 
\begin_inset Formula $q$
\end_inset

.
 Since kernel 
\begin_inset Formula $k$
\end_inset

 is cc-universal this embedding is zero if and only if 
\begin_inset Formula $g=0$
\end_inset

 which implies that 
\begin_inset Formula 
\[
\nabla\log\frac{p(y)}{q(y)}=(0,\cdots,0)
\]

\end_inset

A constant vector filed of derivatives can be generated only by a constant
 functions, so 
\begin_inset Formula $\log\frac{p(y)}{q(y)}=C$
\end_inset

, for some 
\begin_inset Formula $C$
\end_inset

, which implies that 
\begin_inset Formula $p(y)=e^{C}q(y)$
\end_inset

.
 Since 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 integrates to one 
\begin_inset Formula $C=0$
\end_inset

 and so 
\begin_inset Formula $p=q$
\end_inset

.
 
\end_layout

\begin_layout Standard
One practical consideration is choice of flip parameter for the bootstrap
 process when dealing with non-iid data.
 While there is no good reprice for a finite sample and unknown covariance
 structure, in the first experiment we discuss a strategy applicable to
 chins obtained from MCMC methods.
 
\end_layout

\begin_layout Subsection

\lang english
Test
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sub:details_testing"

\end_inset


\end_layout

\begin_layout Standard

\lang english
Basic two concepts required for derivation of the test statistic is 
\begin_inset Formula $\tau$
\end_inset

-mixing 
\begin_inset CommandInset citation
LatexCommand cite
key "dedecker2007weak"

\end_inset

, and 
\begin_inset Formula $V$
\end_inset

-statistics 
\begin_inset CommandInset citation
LatexCommand citet
key "serfling80"

\end_inset

.
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Formula $\tau$
\end_inset

-mixing is a notion of dependence within the observations, weak enough for
 most practical applications.
 Trivially, iid observations are 
\begin_inset Formula $\tau$
\end_inset

-mixing.
 As for the Markov Chains, which convergance we study in the experiments,
 the property of geometric ergodicity implies 
\begin_inset Formula $\tau$
\end_inset

-mixing (given that the stationary distribution has a finite moment, see
 appendix B of 
\begin_inset CommandInset citation
LatexCommand citet
key "chwialkowski2014kernel"

\end_inset

).
 To introduce 
\begin_inset Formula $\tau$
\end_inset

-mixing formally, let 
\begin_inset Formula $\{X_{t},\mathcal{F}_{t}\}_{t\in\mathbb{N}}$
\end_inset

 be a stationary sequence of integrable random variables, with respect to
 a natural filtration 
\begin_inset Formula $\mathcal{F}_{t}$
\end_inset

.
 The sequence is called 
\begin_inset Formula $\tau$
\end_inset

-dependent if 
\begin_inset Formula $\tau(r)$
\end_inset

 converges to zero with 
\begin_inset Formula $r$
\end_inset

 going to infinity, where 
\begin_inset Formula $\tau$
\end_inset

 is defined as follows 
\begin_inset Formula 
\begin{align*}
\tau(r) & =\sup_{l\in\mathbb{N}}\frac{1}{l}\sup_{r\leq i_{1}...\leq i_{l}}T(\mathcal{F}_{0},(X_{i_{1}},...,X_{i_{l}})),\\
T(\mathcal{M},X) & =\ev\left(\sup_{g\in\Lambda}\left|\int g(t)(dP_{X|\mathcal{M}}-dP)\right|\right)
\end{align*}

\end_inset

and 
\begin_inset Formula $\Lambda$
\end_inset

 is the set of all one-Lipschitz continuous real-valued functions on the
 domain of 
\begin_inset Formula $X$
\end_inset

.
 For this work we will assume a technical condition 
\begin_inset Formula $\tau(t)\leq O(t^{-6})$
\end_inset

.
 A 
\begin_inset Formula $V$
\end_inset

-statistic of a function 
\begin_inset Formula $h,$
\end_inset

 is defined as follows
\begin_inset Formula 
\[
V_{n}(h,X)=\frac{1}{n}\sum_{i,j=1}^{n}h(X_{i},X_{j}).
\]

\end_inset

For the one sample test the function 
\begin_inset Formula $h$
\end_inset

 is 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Formula 
\begin{align*}
h(x,y) & =\langle\nabla\log p(X_{1}),\nabla\log p(X_{2})\rangle_{2}k(X_{1},X_{2})\\
 & +\langle\nabla p(X_{2}),\nabla_{1}k(X_{1},X_{2})\rangle_{2}\\
 & +\langle\nabla\log p(X_{1}),\nabla_{2}k(X_{1},X_{2})\rangle_{2}\\
 & +\langle\nabla_{1}k(X_{1},X_{2}),\nabla_{2}k(X_{1},X_{2})\rangle_{2}.
\end{align*}

\end_inset

The distribution of 
\begin_inset Formula $V(h,X)$
\end_inset

 depends whether 
\begin_inset Formula $X\sim p$
\end_inset

 or not.
 This is illustrated by the following theorem, which is a direct application
 of the Theorem 2.1 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset


\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm: null_dist"

\end_inset

If 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $X\sim p$
\end_inset

 then 
\begin_inset Formula $V_{n}(h,X)$
\end_inset

 converges weakly to some distribution.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
Proof, which is a simple verification of the assumptions, is in the appendix.
 Although a formula for a limit distribution of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}(h,X)$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 can be derived explicitly ( 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Theorem 2.1
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

), we do provide it here.
 To our knowledge there are no methods of obtaining quintiles of a limit
 of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}(h,X)$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 in an analytic form.
 The common solution, is estimating quantiles by a resampling method.
 For resampling we use wild bootstrap 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
method 
\emph default
proposed by
\emph on
 
\begin_inset CommandInset citation
LatexCommand cite
key "Shao2010,leucht_dependent_2013"

\end_inset

.
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

 
\end_layout

\begin_layout Theorem

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm:wild_bootstrap_works"

\end_inset

of 
\begin_inset Formula $V_{n}$
\end_inset

.
 Let 
\begin_inset Formula $f(W_{t,n})=\sup_{x}|P(V_{b,n}>x|W_{t,n})-P(V_{n}>x)|$
\end_inset

 be a difference between quantiles.
 
\begin_inset Formula $f(W_{t,n})$
\end_inset

 converges to zero in probability.
 
\end_layout

\begin_layout Proof

\lang english
We have checked the assumption A2 in the proof of the Theorem 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset


\lang english
.
 The assumption B1 follows from our technical assumption 
\begin_inset Formula $\tau(t)\leq O(t^{-6})$
\end_inset

.
 Finally we check assumption B2 (bootstrap assumption):
\emph on
 
\begin_inset Formula $\{W_{t,n}\}_{1\leq t\leq n}$
\end_inset


\emph default
 is a row-wise strictly stationary triangular array independent of all 
\begin_inset Formula $Z_{t}$
\end_inset

 such that 
\begin_inset Formula $\ev W_{t,n}=0$
\end_inset

 and 
\begin_inset Formula $\sup_{n}\ev|W_{t,n}^{2+\sigma}|<\infty$
\end_inset

 for some 
\begin_inset Formula $\sigma>0$
\end_inset

.
 The auto-covariance of the process is given by 
\begin_inset Formula $\ev W_{s,n}W_{t,n}=(1-2p_{n})^{s-t}$
\end_inset

 , so the function 
\begin_inset Formula $\rho(x)=\exp(-x)$
\end_inset

, and 
\begin_inset Formula $l_{n}=\log(1-2p_{n})^{-1}$
\end_inset

.
 We verify that 
\begin_inset Formula $\lim_{u\to0}\rho(u)=1$
\end_inset

, 
\begin_inset Formula $l_{n}=o(n)$
\end_inset

 , 
\begin_inset Formula $\lim_{n\to\infty}l_{n}=\infty$
\end_inset

 and 
\begin_inset Formula $\sum_{r=1}^{n-1}\rho(|r|/l_{n})=O(l_{n})$
\end_inset

.
 
\end_layout

\begin_layout Section

\lang english
Experiments
\end_layout

\begin_layout Standard

\lang english
The proposed test can be used for the convergance diagnostics of the MCMC
 methods.
 In the first two experiments we demonstrate how use the test to verify
 if the samples obtained form a MCMC sampling method can be assumed to be
 drawn from the stationary distribution.
 In the third experiment we
\lang british
 use the test for a different task -- quality check of a density estimator.
\end_layout

\begin_layout Subsubsection*

\lang english
Student's t vs Normal
\end_layout

\begin_layout Standard

\lang english
In this sanity check we modify the experiment 4.1 from 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

.
 The null hypothesis is that the observed samples come from a standard normal
 distribution.
 We study the power of the test against samples from Student's t distribution.
 We expect to observe low p-values when testing against a Student's t distributi
on with few degrees of freedom.
 1, 5, 10 or infinity are degrees of freedom that we consider in this experiment
, where infinity is equivalent to sampling from a standard normal distribution.
 For a fixed number of degrees of freedom we draw 1400 samples and calculate
 the p-value.
 This procedure is repeated one hundred times and the bar plot of p-values
 is plotted (see Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

).
 
\end_layout

\begin_layout Standard
The twist to 
\lang english
the experiment 4.1 by 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

 is that the draws from the Student's t distribution have temporal correlation.
 The samples are generated using Metropolisâ€“Hastings algorithm, with a Gaussian
 random walk (variance equal to 0.5).
 We emphasize a need for an appropriate choice of the bootstrap process
 parameter, 
\begin_inset Formula $p_{n}$
\end_inset

, the probability of a sign flip.
 In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

 we plot p-values for 
\begin_inset Formula $p_{n}$
\end_inset

 being set to 
\begin_inset Formula $50\%$
\end_inset

.
 Such a high value of 
\begin_inset Formula $p_{n}$
\end_inset

 is 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
suitable for iid observations, but results in too conservative p-values
 for temporally correlated observations.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $p_{n}=2\%$
\end_inset

, which gives well calibrated distribution of the p-values under the null
 hypothesis (see box plot for an infinite number degrees of freedom), however
 reduces the power of a the test.
 Indeed, p-values for five degrees of freedom are already large.
 The solution that we recommend is a mixture of thinning and adjusting 
\begin_inset Formula $p_{n},$
\end_inset

 as presented in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

.
 We have thinned the observations by a factor of 20, set 
\begin_inset Formula $p_{n}=10\%$
\end_inset

 and managed to preserve a good power and a right calibration of p-values
 under the null hypothesis.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang english
\begin_inset Graphics
	filename img/sgld_student_bad.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
TODO
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:student_bad"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang english
\begin_inset Graphics
	filename img/sgld_student.pdf

\end_inset

 
\begin_inset Caption

\begin_layout Plain Layout

\lang english
Large auto covariance, and suitable bootstrap.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang english
\begin_inset CommandInset label
LatexCommand label
name "fig:studentst"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/sgld_student_opt.pdf

\end_inset


\begin_inset Caption

\begin_layout Plain Layout

\lang english
Thinned sample, and suitable bootstrap.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:thinning"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*

\lang english
Approximate MCMC algorithm 
\end_layout

\begin_layout Standard

\lang english
Here we multi modal ilustrative model used before in 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring,welling2011bayesian"

\end_inset

.
 The model is 
\begin_inset Formula 
\begin{align*}
\theta_{1}\sim N(0,10);\theta_{2}\sim N(0,1)\\
X_{i}\sim\frac{1}{2}N(\theta_{1},4)+\frac{1}{2}N(\theta_{2},4)
\end{align*}

\end_inset

400 points are drawn from this model.
 The task is to estimate a posteriori distribution of 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Paragraph

\lang english
Metropolis Hastings with random walk
\end_layout

\begin_layout Standard

\lang english
We use plain MH MCMC with Gaussian proposal with with standard deviation
 equal to 
\begin_inset Formula $0.2$
\end_inset

.
\end_layout

\begin_layout Standard

\lang english
Austerity MCMC 
\begin_inset CommandInset citation
LatexCommand citet
key "korattikara2013austerity"

\end_inset

is a Monte Carlo procedure designed to reduce number of likelihood evaluation
 in the acceptance step of the Metropolis-Hastings algorithm.
 The crux of algorithm is to look just at a subset of the data and make
 a acceptance/rejection decision based on this subset.
 The probability of which making a wrong decision is proportional to 
\begin_inset Formula $\epsilon\in[0,1]$
\end_inset

.
 Not surprisingly parameter 
\begin_inset Formula $\epsilon$
\end_inset

 influences time complexity of Austerity MCMC, the larger 
\begin_inset Formula $\epsilon$
\end_inset

, hence toleration for a mistake, the lower computational cost, on average.
 We simulate 
\begin_inset Formula $\{X_{i}\}_{\{1\leq i\leq400\}}$
\end_inset

 points from the model with 
\begin_inset Formula $\theta_{1}=0$
\end_inset

 and 
\begin_inset Formula $\theta_{2}=1$
\end_inset

.
 In such a setting there are two modes in the posteriori distribution, one
 at the the point 
\begin_inset Formula $0,1$
\end_inset

 and the other at the point 
\begin_inset Formula $1,-1$
\end_inset

.
 We run the Austerity algorithm with 
\begin_inset Formula $\epsilon$
\end_inset

 varying in a range 
\begin_inset Formula $[0.001,0.2]$
\end_inset

.
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we calculate individual thinning factor such that correlation between consecuti
ve elements of samples form the chains is smaller than 
\begin_inset Formula $0.5$
\end_inset

 (greater 
\begin_inset Formula $\epsilon$
\end_inset

 require usually more less thinning).
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we then sample 
\begin_inset Formula $\{\theta_{i}\}_{1\leq i\leq500}$
\end_inset

 and calculate a p-value using present one sample test.
 This way we generate 100 p-values for each 
\begin_inset Formula $\epsilon$
\end_inset

 ,the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "p-values"

\end_inset

shows distribution of those p-values as a function of 
\begin_inset Formula $\epsilon$
\end_inset

 -- 
\begin_inset Formula $\epsilon=0.04$
\end_inset

 gives a good approximation of true stationary distribution.
 We also collect statistics about average number of likelihood evaluations,
 these are presented in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "lik-evals"

\end_inset

 (y-axis is in millions of evaluations).
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset CommandInset label
LatexCommand label
name "p-values"

\end_inset


\begin_inset Graphics
	filename img/Heiko1.pdf

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "lik-evals"

\end_inset

 
\begin_inset Graphics
	filename img/Heiko2.pdf

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Convergence in non-parametric density estimation
\end_layout

\begin_layout Standard
Our next experiment illustrates using the developed test to asses estimation
 quality in the context of nonparametricdensity estimation.
 We quantify estimation quality and approximation quality of the infinite
 dimensional exponential family model 
\begin_inset CommandInset citation
LatexCommand citep
key "SriFukKumGreHyv14"

\end_inset

 and its recent random Fourier features approximation 
\begin_inset CommandInset citation
LatexCommand citep
key "strathmann2015gradient"

\end_inset

 respectively.
\end_layout

\begin_layout Standard
The original model's (un-normalised) log pdf is given by 
\begin_inset Formula $f(x)$
\end_inset

 where 
\begin_inset Formula $f\in{\cal H}$
\end_inset

 lies in a Reproducing Kernel Hilbert Space 
\begin_inset Formula ${\cal H}$
\end_inset

 induced by a Gaussian kernel with bandwidth 1.
 We fit the model to 
\begin_inset Formula $N$
\end_inset

 standard Gaussian distributed data and perform our quadratic time test
 on seperate test data of a fixed size 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

.
 We aim to identify the number of samples necessary to make model and data
 indistinguishable for a test of a certain power.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_data"

\end_inset

 shows the distribution of p-values for this particualar test power 
\begin_inset Formula $N_{\text{test}}$
\end_inset

 is uniform for 
\begin_inset Formula $N=5000$
\end_inset

, but already at 
\begin_inset Formula $N=500$
\end_inset

, the null hypothesis would very rarely be rejected.
\end_layout

\begin_layout Standard
We now use the recent random Features approximation 
\begin_inset CommandInset citation
LatexCommand citep
key "strathmann2015gradient"

\end_inset

 where the log pdf is taken to be 
\begin_inset Formula $\theta^{\top}\phi_{x}$
\end_inset

 where 
\begin_inset Formula $\theta\in\mathbb{R}^{m}$
\end_inset

 and 
\begin_inset Formula $\phi_{x}\in\mathbb{R}^{m}$
\end_inset

 is the random Fourier feature embedding 
\begin_inset CommandInset citation
LatexCommand citep
key "Rahimi2007"

\end_inset

.
 The natural question when using this approximation is: 
\begin_inset Quotes eld
\end_inset

How many random features do it I need?
\begin_inset Quotes erd
\end_inset

 Using the same test power 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

 as above, and a large number of available samples 
\begin_inset Formula $N=5\cdot10^{4}$
\end_inset

, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_features"

\end_inset

 shows the distribution of p-values for an increasing number of random features
 
\begin_inset Formula $m$
\end_inset

.
 From about 
\begin_inset Formula $m=50$
\end_inset

, the null hypothesis would rarely be rejected for the chosen test power.
\end_layout

\begin_layout Standard
and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_features"

\end_inset

 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_data_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
P-values for an increasing number of data 
\begin_inset Formula $N$
\end_inset

 for the non-parametric model.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_data"

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_features_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
P-values for an increasing number of random features 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_features"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
git 
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "icml2015"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Part*
Appendix
\end_layout

\begin_layout Section

\lang english
boring proofs
\end_layout

\begin_layout Proof
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt"

\end_inset


\lang english
It is sufficient to check that coefficients of 
\begin_inset Formula $\xi$
\end_inset

 are Bochner integrable (
\begin_inset CommandInset citation
LatexCommand cite
after "Definition A.5.20"
key "SteChr08"

\end_inset

).
 first we check that for random any variable 
\begin_inset Formula $Z$
\end_inset

 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)\right\Vert ^{2}=\ev\left(\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,Z)\right)^{2}<\ev\|\nabla\log p(X)\|^{2}<\infty,
\]

\end_inset

which follows form assumption (i) and boundedness of the kernel.
 Next we check that 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial k(Z,\cdot)}{\partial x}\right\Vert ^{2}=\ev\left(\frac{\partial^{2}k(Z,Z)}{dx_{i}dx_{i+d}}\right)^{2}<\infty,
\]

\end_inset

which follows from assumption (iii).
 
\end_layout

\begin_layout Proof

\lang english
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset

Recall that use notation 
\begin_inset Formula 
\begin{align*}
\nabla_{1}k(x,y)=\left(\frac{\partial k(x,y)}{\partial x_{1}},\cdots,\frac{\partial k(x,y)}{\partial x_{d}}\right)\\
\nabla_{2}k(x,y)=\left(\frac{\partial k(x,y)}{\partial y_{1}},\cdots,\frac{\partial k(x,y)}{\partial y_{d}}\right).\\
\end{align*}

\end_inset

and 
\begin_inset Formula $\langle\cdot,\cdot\rangle_{2}$
\end_inset

 for inner product in 
\begin_inset Formula $R^{d}$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
S(X,\mathcal{F},p)^{2} & =\langle\xi,\xi\rangle_{\mathcal{F}^{d}}\\
 & =\langle\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{1}k(X,\cdot)\right],\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{1}k(X,\cdot)\right]\rangle_{\mathcal{F}^{d}}\\
 & =\ev\langle\nabla\log p(X_{1})k(X_{1},\cdot)+\nabla_{1}k(X_{1},\cdot),\nabla\log p(X_{2})k(\cdot,X_{2})+\nabla_{2}k(\cdot,X_{2})\rangle_{\mathcal{F}^{d}}\\
 & =\ev\langle\nabla\log p(X_{1}),\nabla\log p(X_{2})\rangle_{2}k(X_{1},X_{2})+\ev\langle\nabla p(X_{2}),\nabla_{1}k(X_{1},X_{2})\rangle_{2}\\
 & \quad+\ev\langle\nabla\log p(X_{1}),\nabla_{2}k(X_{1},X_{2})\rangle_{2}+\ev\ \langle\nabla_{1}k(X_{1},X_{2}),\nabla_{2}k(X_{1},X_{2})\rangle_{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Proof

\lang english
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset

The condition A1 is trivially stratified by the assumption 
\begin_inset Formula $\sum_{t=1}^{\infty}\sqrt{\tau(t)}\leq\infty$
\end_inset

.
 The condition A2 (iv), Lipschitz continuity.
 follows form the assumption iv).
 The positive definiteness and degeneracy follows form the proof of th the
 Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem_discrepancy_is_metric"

\end_inset

.
 Indeed
\end_layout

\begin_layout Proof

\lang english
\begin_inset Formula 
\[
h(x,y)=\langle\left[\nabla\log p(x)k(x,\cdot)+\nabla_{1}k(x,\cdot)\right],\left[\nabla\log p(y)k(y,\cdot)+\nabla_{1}k(y,\cdot)\right]\rangle_{\mathcal{F}^{d}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

so it's an inner product and hence positive definite.
 Degeneracy follows form the fact that for ant 
\begin_inset Formula $t$
\end_inset

, by Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

,
\begin_inset Formula $\ev\nabla\log p(x)k(x,t)+\nabla_{1}k(x,t)=0$
\end_inset

.
 Finally the condition A2 (iii), 
\begin_inset Formula $\ev h(X,X)\leq\infty$
\end_inset

 follows form (ii), (iii) and boundlessness of the kernel.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:easy"

\end_inset

 If a random variable 
\begin_inset Formula $X$
\end_inset

 is distributed according to 
\begin_inset Formula $p$
\end_inset

, then for all function 
\begin_inset Formula $f\in\mathcal{F}$
\end_inset

 expected value of 
\begin_inset Formula $T$
\end_inset

 is zero, i.e.
 
\begin_inset Formula $\forall_{f\in\mathcal{F}}\ev(T_{q}f)(X)=0$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
First we show that the functions 
\begin_inset Formula $g_{i}=p\cdot f_{i}$
\end_inset

 vanish at infinity, by which we mean that for all dimensions 
\begin_inset Formula $j$
\end_inset

 
\begin_inset Formula 
\[
\lim_{x_{j}\to\infty}g_{i}(x_{1},\cdots,x_{d})=0.
\]

\end_inset

The density function 
\begin_inset Formula $p$
\end_inset

 vanishes at infinity.
 The function 
\begin_inset Formula $f$
\end_inset

 is bounded, which is implied by Cauchy-Schwarz inequality -- 
\begin_inset Formula $\left|f(x)\right|\le\left\Vert f\right\Vert \sqrt{k(x,x)}$
\end_inset

.
 This implies that the function 
\begin_inset Formula $g$
\end_inset

 vanishes at infinity.
 To show the expected value 
\begin_inset Formula $\ev(T)f(X)$
\end_inset

 is zero, it is sufficient to show that for all dimensions 
\begin_inset Formula $i$
\end_inset

, the expected value of 
\begin_inset Formula $\frac{\partial\log p(X)}{\partial x_{i}}f_{i}(X)+\frac{\partial f_{i}(X)}{\partial x_{i}}$
\end_inset

 is zero.
 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right)\\
 & =\int_{R_{d}}\left[\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{1}{p(x)}\frac{\partial q(x)}{\partial x_{i}}f(x)+\frac{\partial f(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{\partial p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}q(x)\right]dx\\
 & \overset{(a)}{=}\int_{R_{d-1}}\left(\lim_{R\to\infty}p(x)f_{i}(x)\bigg|_{x_{i}=-R}^{x_{i}=R}\right)dx_{1}\cdots dx_{i-1}\cdots dx_{i+1}\cdots d{x_{d}}\\
 & =\int_{R_{d-1}}0dx_{1}\cdots dx_{i-1}\cdots dx_{i+1}\cdots d{x_{d}}\\
 & =0.
\end{align*}

\end_inset

For the equation (a) we have used integration by parts and fact that 
\begin_inset Formula $g_{i}$
\end_inset

 vanishes at infinity.
 
\end_layout

\begin_layout Subsection

\lang english
Linear time
\end_layout

\begin_layout Standard

\lang english
For some fixed location 
\begin_inset Formula $y$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

, define a random variable 
\begin_inset Formula $s(X,y)$
\end_inset

 
\begin_inset Formula 
\begin{align}
s(X,y)=\nabla\log p(X)g(X,y)-\nabla g(X,y).
\end{align}

\end_inset

For some number of random locations 
\begin_inset Formula $Y_{1},Y_{J}$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

 define a random vector 
\begin_inset Formula $Z_{i}$
\end_inset

 
\begin_inset Formula 
\begin{equation}
Z_{i}=(s(X_{i},Y_{1}),\cdots,s(X_{i},Y_{J}))\in\mathbf{R}^{J}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\lang english
Let 
\begin_inset Formula $W_{n}$
\end_inset

 be a mean of 
\begin_inset Formula $Z_{i}$
\end_inset

's 
\begin_inset Formula $W_{n}=\frac{1}{n}\sum_{i=1}^{n}Z_{i},$
\end_inset

 and 
\begin_inset Formula $\Sigma_{n}$
\end_inset

 its covariance matrix 
\begin_inset Formula $\Sigma_{n}=\frac{1}{n}ZZ^{T}$
\end_inset

.
 The test statistic is 
\begin_inset Formula 
\begin{equation}
S_{n}=nW_{n}\Sigma_{n}^{-1}W_{n}.
\end{equation}

\end_inset

The computation of 
\begin_inset Formula $S_{n}$
\end_inset

 requires inversion of a 
\begin_inset Formula $J\times J$
\end_inset

 matrix 
\begin_inset Formula $\Sigma_{n}$
\end_inset

, but this is fast and numerically stable: 
\begin_inset Formula $J$
\end_inset

 will typically be small, and is less than 10 in our experiments.
 The next proposition demonstrates the use of 
\begin_inset Formula $S_{n}$
\end_inset

 as a one-sample test.
\end_layout

\begin_layout Proposition

\lang english
\begin_inset Argument
status open

\begin_layout Plain Layout

\lang english
Asymptotic behavior of 
\begin_inset Formula $S_{n}$
\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "prop:Hotelling"

\end_inset

 If 
\begin_inset Formula $\ev s(X,y)=0$
\end_inset

 for all 
\begin_inset Formula $y$
\end_inset

, then the statistic 
\begin_inset Formula $S_{n}$
\end_inset

 is a.s.
 asymptotically distributed as a 
\begin_inset Formula $\chi^{2}$
\end_inset

-random variable with 
\begin_inset Formula $Jd$
\end_inset

 degrees of freedom, where 
\begin_inset Formula $d$
\end_inset

 is 
\begin_inset Formula $X$
\end_inset

 dimensionality (as 
\begin_inset Formula $n\to\infty$
\end_inset

 with 
\begin_inset Formula $d$
\end_inset

 fixed).
 If 
\begin_inset Formula $\ev s(X,y)\neq0$
\end_inset

 for almost all 
\begin_inset Formula $y$
\end_inset

 then a.s.
 for any fixed 
\begin_inset Formula $r$
\end_inset

, 
\begin_inset Formula $\mathbb{P}(S_{n}>r)\to1$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

 .
 
\end_layout

\begin_layout Paragraph

\lang english
One sample test
\end_layout

\begin_layout Standard

\lang english
Calculate 
\begin_inset Formula $S_{n}$
\end_inset

.
 Choose a threshold 
\begin_inset Formula $r_{\alpha}$
\end_inset

 corresponding to the 
\begin_inset Formula $1-\alpha$
\end_inset

 quantile of a 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution with 
\begin_inset Formula $J$
\end_inset

 degrees of freedom, and reject the null hypothesis whenever 
\begin_inset Formula $S_{n}$
\end_inset

 is larger than 
\begin_inset Formula $r_{\alpha}$
\end_inset

.
 
\end_layout

\end_body
\end_document
