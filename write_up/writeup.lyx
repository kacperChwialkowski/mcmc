#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
% from the icml 2016 example tex file
\usepackage{times}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2016} 

%\usepackage[accepted]{icml2016}


\newcommand{\heiko}[1]{   {\bf \color{blue}{HS: #1}}  }
\newcommand{\kacper}[1]{   {\bf \color{red}{K: #1}}  }
\newcommand{\arthur}[1]{   {\bf \color{magenta}{AG: #1}}  }

%\newcommand{\heiko}[1]{}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[ 
\backslash
icmltitle{A Kernel Test of Goodness of Fit}
\end_layout

\begin_layout Plain Layout

% It is OKAY to include author information, even for blind
\end_layout

\begin_layout Plain Layout

% submissions: the style file will automatically remove it for you
\end_layout

\begin_layout Plain Layout

% unless you've provided the [accepted] option to the icml2015
\end_layout

\begin_layout Plain Layout

% package.
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Kacper Chwialkowski$^*$}{kacper.chwialkowski@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Heiko Strathmann$^*$}{heiko.strathmann@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Arthur Gretton}{arthur.gretton@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Gatsby Unit, University College London, United Kingdom}
\end_layout

\begin_layout Plain Layout

% You may provide any keywords that you 
\end_layout

\begin_layout Plain Layout

% find helpful for describing your paper; these are used to populate
\end_layout

\begin_layout Plain Layout

% the "keywords" metadata in the PDF but will not be shown in the document
\end_layout

\begin_layout Plain Layout


\backslash
icmlkeywords{kernel methods, goodness-of-fit, Stein's method, statistical
 testing}
\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Abstract
We propose a nonparametric statistical test for goodness-of-fit: given a
 set of samples, the test determines how likely it is that these were generated
 from a target density function.
 The measure of goodness-of-fit is a divergence constructed via Stein's
 method using the functions from a Reproducing Kernel Hilbert Space.
 Our test statistic is based on an empirical estimate of this divergence,
 taking the form of a V-statistic in terms of the log gradients of the target
 density and the kernel.
 We derive a statistical test, both for i.i.d.
 and non-i.i.d.
 samples, where in the latter setting we make use of a wild bootstrap procedure.
 We apply our test to quantifying convergence of approximate Markov Chain
 Monte Carlo methods, statistical model critisism, and evaluating quality
 of fit vs model complexity in nonparametric density estimation.
\end_layout

\begin_layout Standard

\lang english
\begin_inset FormulaMacro
\newcommand{\ev}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Goodness-of-fit testing, or measuring sampling quality, is a fundamental
 tool in statistical analysis, dating back to the test of Kolmogorov and
 Smirnov 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolmogorov33,Smirnov48"

\end_inset

.
 Given a set of samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

 with distribution 
\begin_inset Formula $Z_{i}\sim q$
\end_inset

, our interest is in whether 
\begin_inset Formula $q$
\end_inset

 matches some reference or target distribution 
\begin_inset Formula $p$
\end_inset


\lang english
, which we assume to be only known up to the normalisation constant.
\end_layout

\begin_layout Standard

\lang english
Our measure of goodness of fit builds on recent work of 
\lang british

\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

, who proposed an elegant measure of sample quality with respect to a target.
 This is a maximum discrepancy between empirical sample expectations and
 target expectations over a large class of test functions, constructed so
 as to have zero expectation over the target distribution by use of a Stein
 operator.
 This operator depends only on the derivarive of the 
\begin_inset Formula $\log q$
\end_inset

: thus, the approach can be applied very generally, as it does not require
 closed-form integrals over the target distribution (or numerical approximations
 of such integrals).
 By contrast, many earlier discrepancy measures require integrals with resepect
 to the target (see below for a review).
 This is problematic if the intention is to perform benchmarks for assessing
 Markov Chain Monte Carlo, since these integrals will certainly not be known
 to the practitioner.
\end_layout

\begin_layout Standard
A challenge in applying the approach of 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

 is the complexity of the function class used, which results from applying
 the Stein operator to the bounded Lipschitz functions (which give rise
 to the Wasserstein integral probability metric).
 Thus, their sample quality measure requires solving an expensive linear
 program that arises from a complicated construction of graph Stein discrepancie
s and geometric spanners.
 Their metric furthermore requires access to nontrivial lower bounds that,
 despite being provided for log-concave densities, are a largely open problem
 otherwise, in particular for multivariate cases.
\end_layout

\begin_layout Standard
An important application of a goodness-of-fit measure is in statistical
 testing, where it is desired to determine whether the measure is large
 enough to reject the null hypothesis (that the sample arises from the target
 distribution).
 One approach is to establish the asymptotic behaviour of the test statistic,
 and to set a test threshold at a large quantile of he asymptotic distribution.
 The asymptotic behaviour of the Wasserstein-based Stein discrepancy remains
 a challenging open problem due to the complexity of the function class
 used.
 Thus, it is not clear how one would compute p-values for this statistic,
 or determine when the goodness of fit allows one to accept the null hypothesis
 (at the user-specified test level).
\end_layout

\begin_layout Standard
The key contribution of this work is to define a statistical test of goodness-of
-fit, based on a Stein discrepancy computed in a Reproducing Kernel Hilbert
 Space (RKHS).
 To construct our test statistic, we apply the Stein operator to our chosen
 RKHS functions, and define our measure of goodness of fit as the largest
 discrepancy between empirical sample expectations and target expectations
 over the resulting test functions.
 This approach is a natural extension to goodness-of-fit testing of the
 earlier two-sample tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton2012kernel"

\end_inset

 and independence tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton_kernel_2008"

\end_inset

 based on the maximum mean discrepancy, which is an integral probability
 metric.
 As with these earlier tests, our statistic is a simple U-statistic, and
 can be computed in closed form and in quadratic time; moreover, it is an
 unbiased estimate of the corresponding population discrepancy.
 As with all Stein-based discrepancies, only the gradient of the log-density
 of the target density is needed; we do not require integrals with respect
 to the target density -- including the normalisation constant.
 Given that our test statistic is a V-statistic, we may make use of the
 extensive literature on asymptotics of V-statistics to formulate a hypothesis
 test 
\begin_inset CommandInset citation
LatexCommand citep
key "serfling80,leucht_dependent_2013"

\end_inset

.
 We are able to provide statistical tests even in the case of correlated
 samples, which is essential if the test is to be used in assessing the
 quality of output of an MCMC procedure.
\end_layout

\begin_layout Standard
Several alternative approaches exist in the statistics literature to perform
 goodness-of-fit testing.
 A first strategy is to partition the space, and to conduct the test on
 a histogram estimate of the distribution 
\begin_inset CommandInset citation
LatexCommand citep
key "Bar89,Beirlant2,Gyorfi,GyVa02"

\end_inset

.

\lang english
 Such space parititioning approaches can have attractive theoretical properties
 (eg distribution-free test thresholds) and work well in low dimensions,
 however they are much less powerful than alternatives once the dimensionality
 increases 
\begin_inset CommandInset citation
LatexCommand cite
key "GreGyo10"

\end_inset

.

\lang british
 A second popular approach has been to use the smoothed 
\begin_inset Formula $L_{2}$
\end_inset

 distance between the empirical chacacteristic function of the sample, and
 the characteristic function of the target density.
 This dates back to the test of Gaussianity of 
\begin_inset CommandInset citation
LatexCommand citet
key "BaringhausHenze88"

\end_inset

, who used a squared exponential smoothing function (see Eq.
 2.1 in their paper).
 For this choice of smoothing function, their statistic is identical to
 the maximum mean discrepancy (MMD) with the squared exponential kernel,
 which can be shown using the Bochner representation of the kernel (compare
 with 
\begin_inset CommandInset citation
LatexCommand citealt
after "Corollary 4"
key "SriGreFukLanetal10"

\end_inset

).
 It is essential in this case that the target distribution be Gaussian,
 since the convolution with the kernel (or in the Fourier domain, the smoothing
 function) must be available in closed form.
 An 
\begin_inset Formula $L_{2}$
\end_inset

 distance between Parzen window estimates can also be used  
\begin_inset CommandInset citation
LatexCommand citep
key "BowFos93"

\end_inset

, giving the same expression again, although the optimal choice of bandwidth
 for consistent Parzen window estimates may not be a good choice for testing
 
\begin_inset CommandInset citation
LatexCommand citep
key "AndHalTit94"

\end_inset

.
 A different smoothing scheme in the frequency domain results in an energy
 distance statistic (this likewise being an MMD with a particular choice
 of kernel 
\begin_inset CommandInset citation
LatexCommand citep
key "SejSriGreFuk13"

\end_inset

), which can be used in a test of normality 
\begin_inset CommandInset citation
LatexCommand citep
key "SzeRiz05"

\end_inset

.
 The key point is that the required integrals are again computable in closed
 form for the Gaussian (this may be extended to certain other families of
 interest, e.g.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Rizzo09"

\end_inset

).
 The requirement of computing closed-form integrals with respect to the
 test distribution severely restricts this testing strategy.
 Recently, an empirical approach to quantifying goodness-of-fit for generative
 models has been proposed 
\begin_inset CommandInset citation
LatexCommand citep
key "lloyd2015statistical"

\end_inset

.
 Via generating samples from a fitted generative models, it is possible
 perform model criticism by quantifying the MMD distance of the generative
 model's distribution and samples used to fit the latter.
 Besides being impractical when a fitted model cannot be sampled from easily,
 it is not clear how to select the number of samples to be generated for
 the test.
 Our methodology elegantly side-steps both issues as no samples from the
 target density are required.
\end_layout

\begin_layout Standard
A specific application of this goodness-of-fit test is that it is able to
 correctly handle samples from approximate Markov Chain Monte Carlo (MCMC)
 
\begin_inset CommandInset citation
LatexCommand citep
key "Korattikara2014,Welling2011,Bardenet2014"

\end_inset

.
 With the hope to increase the overall efficiency, these methods use modificatio
ns to Markov transition kernels that improve mixing speed at the cost of
 introducing an asymptotic bias.
 Such bias-variance trade-offs can usually be tuned with parameters of the
 sampling algorithms.
 It is therefore an important question whether for a particular parameter
 setting and for a fixed run-time, the produced samples have increased in
 quality or not.
 This question cannot be answered with classical MCMC convergence statistics
 alone, such as the widely used potential scale reduction factor (R-factor)
 
\begin_inset CommandInset citation
LatexCommand citep
key "gelman1992inference"

\end_inset

 or the effective sample size -- those assume that the Markov chain reaches
 its equilibrium distribution.
 The described goodness-of-fit testing framework, however, fits very well
 into this context as it exactly quantifies the asymptotic bias of approximate
 MCMC.
 
\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
We begin our presentation in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-Kernel-Goodness-of-fit"

\end_inset

 with a high-level construction of the divergence and a statistical test.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

, we provide additional details and prove the main results presented in
 the previous section.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:experiment"

\end_inset

 contains experimental illustrations on synthetic examples, statistical
 model criticism, bias-variance trade-offs in approximate MCMC, and convergence
 in non-parametric density estimation.
\end_layout

\begin_layout Section

\lang english
A Kernel Goodness-of-fit Test
\begin_inset CommandInset label
LatexCommand label
name "sec:A-Kernel-Goodness-of-fit"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We now give a high-level construction of our divergence discrepancy and
 the statistical test construction.
 While this section aims to communicate the main ideas, we provide details
 and proofs in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

.
\end_layout

\begin_layout Subsection

\lang english
Stein Operator in RKHS
\end_layout

\begin_layout Standard

\lang english
Our goal is to write the maximum discrepancy between target distribution
 
\begin_inset Formula $p$
\end_inset

 and observed sample distribution 
\begin_inset Formula $q$
\end_inset

 in a RKHS.
 Denote by 
\begin_inset Formula ${\cal F}$
\end_inset

 the RKHS of real-valued functions with reproducing kernel 
\begin_inset Formula $k$
\end_inset

, and by 
\begin_inset Formula ${\cal F}^{d}$
\end_inset

 the product RKHS consisting of elements 
\begin_inset Formula $f:=(f_{1},\dots,f_{d})$
\end_inset

 with 
\begin_inset Formula $f_{i}\in{\cal F}$
\end_inset

, and with a standard inner product.
 Similarly to 
\begin_inset CommandInset citation
LatexCommand citet
key "stein1972,gorham2015measuring"

\end_inset

, we begin by defining a Stein operator 
\begin_inset Formula $T$
\end_inset

 acting on 
\begin_inset Formula $f\in\mathcal{F}^{d}$
\end_inset

 
\begin_inset Formula 
\[
Tf:=\sum_{i=1}^{d}\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}.
\]

\end_inset

Suppose a random variable 
\begin_inset Formula $Z$
\end_inset

 is distributed according to a measure
\begin_inset Foot
status open

\begin_layout Plain Layout
Throughout the article, all occurences of 
\begin_inset Formula $Z$
\end_inset

, e.g.
 
\begin_inset Formula $Z',Z_{i},Z_{\heartsuit}$
\end_inset

, are assumed to be distributed to 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\end_inset

 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is distributed according to the target measure 
\begin_inset Formula $p$
\end_inset

.
 As we will see, the operator can be expressed by defining a function that
 depends on gradients of the log-density and the kernel 
\begin_inset Formula 
\begin{equation}
\xi(x,\cdot):=\left[\nabla\log p(x)k(x,\cdot)+\nabla k(x,\cdot)\right],\label{eq:xi}
\end{equation}

\end_inset

whose inner product with 
\begin_inset Formula $f$
\end_inset

 gives exactly the expected value of the Stein operator 
\begin_inset Formula 
\[
\ev Tf(Z)=\langle f,\ev\xi(Z)\rangle_{{\cal F}^{d}}=\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(Z)\rangle_{{\cal F}},
\]

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

.
 On the other hand, for 
\begin_inset Formula $X$
\end_inset

 from the target measure, we have 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

, which can be seen by integration by parts, c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

.
 We can now define a Stein discrepancy and express it in the RKHS,
\begin_inset Formula 
\begin{align*}
S(Z) & :=\sup_{\Vert f\Vert<1}\ev(Tf)(Z)-\ev(Tf)(X)\\
 & =\sup_{\Vert f\Vert<1}\langle f,\ev\xi(Z)-\ev\xi(X)\rangle\\
 & =\sup_{\Vert f\Vert<1}\langle f,\ev\xi(Z)\rangle\\
 & =\|\ev\xi(Z)\|,
\end{align*}

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:discprepancy_maximised_by_norm"

\end_inset

.
 This makes it clear why 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

 is a desirable property: we can compute 
\begin_inset Formula $S(Z)$
\end_inset

 by only computing 
\begin_inset Formula $\|\ev\xi(Z)\|$
\end_inset

 -- without the need to access 
\begin_inset Formula $X$
\end_inset

 in the form of samples from 
\begin_inset Formula $p$
\end_inset

.
 We arrive at our first main result, which states that the above discrepancy
 can be used to distinguish two distributions 
\begin_inset Formula $p,q\in{\cal P}$
\end_inset

.
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "theorem_discrepancy_is_metric"

\end_inset

 Let 
\begin_inset Formula $q,p\in\mathcal{P}$
\end_inset

 and let 
\begin_inset Formula $Z\sim q$
\end_inset

.
 Then 
\begin_inset Formula $S(Z)=0$
\end_inset

 if and only if 
\begin_inset Formula $p=q$
\end_inset

.
 
\end_layout

\begin_layout Standard
Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

 contains minor details, assumptions on 
\begin_inset Formula ${\cal P}$
\end_inset

, and a proof.
 The following theorem gives a simple closed form expression.
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "th:closed_form_discrepancy"

\end_inset

 Let 
\begin_inset Formula 
\begin{align*}
h(x,y) & :=\nabla\log p(x)^{\top}\nabla\log p(y)k(x,y)\\
 & \quad+\nabla\log p(y)^{\top}\nabla_{x}k(x,y)\\
 & \quad+\nabla\log p(x){}^{\top}\nabla_{y}k(x,y)\\
 & \quad+\nabla_{x}k(x,y)^{\top}\nabla_{y}k(x,y).
\end{align*}

\end_inset

The 
\lang british
squared Stein discrepancy is
\lang english
 
\begin_inset Formula $S(Z)^{2}=\ev h(Z,Z')$
\end_inset

.
\end_layout

\begin_layout Standard
We now proceed with constructing an estimator for 
\begin_inset Formula $S(Z)^{2}$
\end_inset

, and outline its asymptotic properties.
\end_layout

\begin_layout Subsection
Wild Bootstrap Testing
\end_layout

\begin_layout Standard
It is straight forward to estimate the squared Stein discrepancy 
\begin_inset Formula $S(Z)^{2}$
\end_inset

 from samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

: a quadratic time estimator is an average of pairwise terms, so called
 V-Statistic, which takes the form
\begin_inset Formula 
\[
V_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}h(Z_{i},Z_{j}).
\]

\end_inset

 The asymptotic null distribution of the normalised V-Statistic 
\begin_inset Formula $nV_{n}$
\end_inset

, however, has no computable closed form.
 Furthermore, care has to be taken when the 
\begin_inset Formula $Z_{i}$
\end_inset

 exhibit correlation structure, as the null distribution significantly changes,
 impacting test significance.
 The wild-bootstrap technique 
\begin_inset CommandInset citation
LatexCommand citep
key "Shao2010,leucht_dependent_2013"

\end_inset

 addresses both problems.
 First, it allows to simulate from the null distribution to compute test
 thresholds.
 Second, it accounts for correlation structure in the 
\begin_inset Formula $Z_{i}$
\end_inset

 by mimicing it with an 
\lang english
auxiliary
\lang british
 random process: a
\lang english
 Markov chain taking values in 
\begin_inset Formula $\{-1,1\}$
\end_inset

, starting from 
\begin_inset Formula $W_{1,n}=1$
\end_inset

,
\lang british

\begin_inset Formula 
\[
W_{t,n}=\mathbf{1}(U_{t}>a_{n})W_{t-1,n}-\mathbf{1}(U_{t}<a_{n})W_{t-1,n},
\]

\end_inset


\lang english
where the 
\begin_inset Formula $U_{t}$
\end_inset

 are uniform i.i.d.
 random variables and 
\begin_inset Formula $a_{n}$
\end_inset

 is the probability of 
\begin_inset Formula $W_{t,n}$
\end_inset

 changing sign.
 This leads to a bootstrapped V-statistic 
\end_layout

\begin_layout Standard

\emph on
\lang english
\begin_inset Formula 
\[
B_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}W_{i,n}W_{j,n}h(Z_{i,}Z_{j}).
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 establishes that, under the null hypothesis, 
\begin_inset Formula $nB_{n}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
 is a good approximation
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
of 
\begin_inset Formula $nV_{n}$
\end_inset

, so it is possible to approximate quantiles of the null distribution by
 samplig from it.
 Under the alternative, however, 
\begin_inset Formula $V_{n}$
\end_inset

 dominates 
\begin_inset Formula $B_{n}$
\end_inset

 -- resulting in almost sure rejection of the null hypothesis.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
We propose the following test
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 procedure for testing the null hypothesis that the 
\begin_inset Formula $Z_{i}$
\end_inset

 are distributed according to the target distribution 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate 
\lang english
the test statistic 
\begin_inset Formula $V_{n}$
\end_inset

.
\end_layout

\begin_layout Itemize

\lang english
Bootstrap
\lang british
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\{B_{n}\}_{i=1}^{D}$
\end_inset

 and estimate the 
\begin_inset Formula $1-\alpha$
\end_inset

 empirical quantile.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
If 
\begin_inset Formula $V_{n}$
\end_inset

 exceeds the quantile, reject.
\end_layout

\begin_layout Section

\lang english
Details
\begin_inset CommandInset label
LatexCommand label
name "sec:Details"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We now prove the claims made in the previous Section.
\end_layout

\begin_layout Subsection

\lang english
Stein Operator in RKHS
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sec:details_kernel_stein"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We make the following assumptions.
 Let 
\begin_inset Formula ${\cal P}$
\end_inset

 be a family of distributions on a real coordinate space, where its elements
 
\begin_inset Formula $p\in\mathcal{P}$
\end_inset

 satisfy two conditions:
\end_layout

\begin_layout Enumerate

\lang english
\begin_inset Argument item:1
status open

\begin_layout Plain Layout

\lang english
(i)
\end_layout

\end_inset


\begin_inset Formula $\nabla\log p(x)$
\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
is Lipschitz continuous.
\end_layout

\begin_layout Enumerate

\lang english
\begin_inset Argument item:1
status open

\begin_layout Plain Layout

\lang english
(ii)
\end_layout

\end_inset


\begin_inset Formula $\ev\|\nabla\log p(Z)\|^{2}\leq\infty$
\end_inset

 for any random variable 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard

\lang english
The kernels considered in this work are assumed to be bounded, symmetric
 and cc-universal 
\begin_inset CommandInset citation
LatexCommand citep
key "SriFukLan11"

\end_inset

.
 On top of that, we assume that the kernel 
\begin_inset Formula $k$
\end_inset

 satisfies
\end_layout

\begin_layout Enumerate

\lang english
\begin_inset Argument item:1
status open

\begin_layout Plain Layout

\lang english
(iii)
\end_layout

\end_inset


\begin_inset Formula $\ev\left(\frac{\partial^{2}k(Z,Z)}{dx_{i}dx_{i+d}}\right)^{2}<\infty$
\end_inset

, for any random variable 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Enumerate

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
\begin_inset Argument item:1
status open

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
(iv)
\end_layout

\end_inset


\begin_inset Formula $\nabla k(x,y)$
\end_inset

 is Lipschitz continuous.
\end_layout

\begin_layout Standard
While (i) and (iv) are a technical assumption necessary for Proposition
 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset

 regarding the wild-bootstrap procedure, (ii) and (iii) are required for
 Bocher integrability of 
\begin_inset Formula $\xi$
\end_inset

 in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt1"

\end_inset

.

\lang english
 
\end_layout

\begin_layout Standard

\lang english
We first show that the expected value of the Stein operator is zero on the
 target measure.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:easy"

\end_inset

 If a random variable 
\begin_inset Formula $X$
\end_inset

 is distributed according to 
\begin_inset Formula $p$
\end_inset

, then for all 
\begin_inset Formula $f\in\mathcal{F}$
\end_inset

, the expected value of 
\begin_inset Formula $T$
\end_inset

 is zero, i.e.
 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
First, we show that the functions 
\begin_inset Formula $g_{i}:=pf_{i}$
\end_inset

 vanish at infinity, i.e.
 for all dimensions 
\begin_inset Formula $j$
\end_inset

, 
\begin_inset Formula 
\[
\lim_{x_{j}\to\infty}g_{i}(x_{1},\cdots,x_{d})=0.
\]

\end_inset

The density function 
\begin_inset Formula $p$
\end_inset

 vanishes at infinity.
 The function 
\begin_inset Formula $f$
\end_inset

 is bounded, which is implied by Cauchy-Schwarz inequality -- 
\begin_inset Formula $\left|f(x)\right|\le\left\Vert f\right\Vert \sqrt{k(x,x)}$
\end_inset

.
 This implies that the function 
\begin_inset Formula $g$
\end_inset

 vanishes at infinity.
 To show that the expected value 
\begin_inset Formula $\ev(T)f(X)$
\end_inset

 is zero, it is sufficient to show that for all dimensions 
\begin_inset Formula $i$
\end_inset

, the expected value of 
\begin_inset Formula $\frac{\partial\log p(X)}{\partial x_{i}}f_{i}(X)+\frac{\partial f_{i}(X)}{\partial x_{i}}$
\end_inset

 is zero.
 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right)\\
 & =\int_{R_{d}}\left[\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{1}{p(x)}\frac{\partial q(x)}{\partial x_{i}}f(x)+\frac{\partial f(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{\partial p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}q(x)\right]dx\\
 & \overset{(a)}{=}\int_{R_{d-1}}\left(\lim_{R\to\infty}p(x)f_{i}(x)\bigg|_{x_{i}=-R}^{x_{i}=R}\right)dx\\
 & =\int_{R_{d-1}}0dx=0
\end{align*}

\end_inset

For the equation (a) we have used integration by parts and fact that 
\begin_inset Formula $g_{i}$
\end_inset

 vanishes at infinity.
 
\end_layout

\begin_layout Standard

\lang english
The following lemmas are useful in proving main results, Theorems 
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:WellDefined"

\end_inset

 
\begin_inset Formula $\xi(x,\cdot)$
\end_inset

 (see 
\lang british
Eq.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:xi"

\end_inset


\lang english
) is an element of the reproducing kernel Hilbert space 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

.
 
\end_layout

\begin_layout Proof

\lang english
We use the proof of 
\begin_inset CommandInset citation
LatexCommand citet
after "Corollary 4.36"
key "SteChr08"

\end_inset

 to see that for all 
\begin_inset Formula $x\in R^{d}$
\end_inset

 each entry of 
\begin_inset Formula $\nabla k(x,\cdot)$
\end_inset

 belongs to 
\begin_inset Formula $\mathcal{F}$
\end_inset

.
 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}k(x,\cdot)\in\mathcal{F}$
\end_inset

, since 
\begin_inset Formula $k(x,\cdot)\in\mathcal{F}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}$
\end_inset

 is a scalar.
 
\end_layout

\begin_layout Standard
The following lemma shows that the expected value of 
\begin_inset Formula $\xi$
\end_inset

 is well defined -- it is needed for establishing a link between Stein operator
 
\begin_inset Formula $Tf$
\end_inset

 and 
\begin_inset Formula $\xi$
\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:BochnerInt1"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

 is is element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

 (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\xi$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is Bochner integrable wrt measure of 
\begin_inset Formula $Z$
\end_inset

).
 
\end_layout

\begin_layout Proof

\lang english
It is sufficient to check that coefficients of 
\begin_inset Formula $\xi$
\end_inset

 are Bochner integrable (
\begin_inset CommandInset citation
LatexCommand cite
after "Definition A.5.20"
key "SteChr08"

\end_inset

).
 First we check that for random any variable 
\begin_inset Formula $Z$
\end_inset

 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)\right\Vert ^{2}<C\ev\|\nabla\log p(X)\|^{2}<\infty,
\]

\end_inset

for some constant 
\begin_inset Formula $C$
\end_inset

, which follows from assumption (i) and boundedness of the kernel.
 Next we check that 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial k(Z,\cdot)}{\partial x}\right\Vert ^{2}=\ev\left(\frac{\partial^{2}k(Z,Z)}{dx_{i}dx_{i+d}}\right)^{2}<\infty,
\]

\end_inset

which follows from assumption (iii).
\end_layout

\begin_layout Standard
Now we can show that the expected value of the Stein operator is a functional
 in the RKHS 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

, hence, it coincidences with an inner product with some element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

.
 This element is the expected value of 
\begin_inset Formula $\xi$
\end_inset

.
 
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:SteinIsInner"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, the expected value of the Stein operator coincides with the inner product
 of 
\begin_inset Formula $f$
\end_inset

 and the expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
\ev Tf(Z)=\langle f,\ev\xi(Z)\rangle_{\mathcal{F}^{d}} & =\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(Z)\rangle_{\mathcal{F}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof

\lang english
We write
\begin_inset Formula 
\begin{align*}
 & \left\langle f_{i},\ev\xi_{i}(Z)\right\rangle _{\mathcal{F}}\\
 & =\left\langle f_{i},\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right]\right\rangle _{\mathcal{F}}\\
 & =\ev\left\langle f_{i},\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right\rangle _{\mathcal{F}}\\
 & =\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}f_{i}(Z)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right].
\end{align*}

\end_inset

The second equality follows from the fact that a linear operator 
\begin_inset Formula $\langle f_{i},\cdot\rangle_{\mathcal{F}}$
\end_inset

 can be interchanged with the Bochner integral, and the fact that 
\begin_inset Formula $\xi$
\end_inset

 is Bochner integrable (Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt1"

\end_inset

).
 The last equality is an application of the reproducing property.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
From the inner product representation, we get  
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:discprepancy_maximised_by_norm"

\end_inset

The discrepancy 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximised by the expected value of 
\begin_inset Formula $\xi$
\end_inset

, i.e.
 
\begin_inset Formula $S(Y,\mathcal{F},p)=\|\ev\xi(Y)\|$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
By the Lemma 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\ev Tf(Y)=\langle f,\ev\xi(Y)\rangle$
\end_inset

 and therefore, 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximized by 
\begin_inset Formula $\frac{\ev\xi(Y)}{\|\ev\xi(Y)\|}$
\end_inset

.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
We are now ready for the proof
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 of the closed form formula for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $S(Y,\mathcal{F},p)^{2}$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset


\end_layout

\end_inset

 We use the notation 
\begin_inset Formula 
\begin{align*}
\nabla_{x}k(x,y)=\left(\frac{\partial k(x,y)}{\partial x_{1}},\cdots,\frac{\partial k(x,y)}{\partial x_{d}}\right)\\
\nabla_{y}k(x,y)=\left(\frac{\partial k(x,y)}{\partial y_{1}},\cdots,\frac{\partial k(x,y)}{\partial y_{d}}\right),
\end{align*}

\end_inset

A plain algebraic manuipulation reveals 
\begin_inset Formula 
\begin{align*}
 & S(X,\mathcal{F},p)^{2}=\langle\xi,\xi\rangle_{\mathcal{F}^{d}}\\
 & =\langle\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X,\cdot)\right],\\
 & \quad\quad\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X,\cdot)\right]\rangle\\
 & =\ev\langle\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X_{1},\cdot),\\
 & \quad\quad\quad\nabla\log p(X)k(\cdot,X)+\nabla_{y}k(\cdot,X)\rangle\\
 & =\ev\nabla\log p(X)^{\top}\nabla\log p(X')k(X,X')\\
 & \quad+\ev\nabla\log p(X)\nabla_{x}k(X,X')\\
 & \quad+\ev\nabla\log p(X){}^{\top}\nabla_{y}k(X,X')\\
 & \quad+\ev\nabla_{x}k(X,X')^{\top}\nabla_{y}k(X,X').
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Finally, we prove that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
the discrepancy 
\begin_inset Formula $S$
\end_inset

 discriminates different probability measures.
 
\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset


\end_layout

\end_inset

 If 
\begin_inset Formula $p=q$
\end_inset

 then 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 by the Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

.
 Suppose 
\begin_inset Formula $p\neq q$
\end_inset

, but 
\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

.
 If 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

 then 
\begin_inset Formula $\ev\xi(Y)=0.$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 For each dimension of 
\begin_inset Formula $\ev\xi(Y)$
\end_inset

, we add and subtract 
\begin_inset Formula $\log q(Y)$
\end_inset

 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial}{\partial x_{i}}\log p(Y)k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & =\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & \quad+\ev\left(\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)\right).
\end{align*}

\end_inset

We have used Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

 to see that 
\begin_inset Formula 
\[
\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)=0.
\]

\end_inset

We recognise that the expected value of 
\begin_inset Formula $\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)$
\end_inset

 is mean the embedding of a function 
\begin_inset Formula $g(y)=\frac{\partial}{\partial x_{i}}\left(\log\frac{p(y)}{q(y)}\right)$
\end_inset

 with respect to the measure 
\begin_inset Formula $q$
\end_inset

.
 Since the kernel 
\begin_inset Formula $k$
\end_inset

 is cc-universal, this embedding is zero if and only if 
\begin_inset Formula $g=0$
\end_inset

, which implies that 
\begin_inset Formula 
\[
\nabla\log\frac{p(y)}{q(y)}=(0,\cdots,0).
\]

\end_inset

A constant vector field of derivatives can only be generated by a constant
 function, so 
\begin_inset Formula $\log\frac{p(y)}{q(y)}=C$
\end_inset

, for some 
\begin_inset Formula $C$
\end_inset

, which implies that 
\begin_inset Formula $p(y)=e^{C}q(y)$
\end_inset

.
 Since 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 both integrate to one, 
\begin_inset Formula $C=0$
\end_inset

 and so 
\begin_inset Formula $p=q$
\end_inset

 -- a contradiction.
\end_layout

\begin_layout Subsection
Wild Bootstrap Testing
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sub:details_testing"

\end_inset


\end_layout

\begin_layout Standard

\lang english
The two concepts required to derive the distribution of the test statistic
 are: 
\begin_inset Formula $\tau$
\end_inset

-mixing 
\begin_inset CommandInset citation
LatexCommand citep
key "dedecker2007weak,leucht_dependent_2013"

\end_inset

, and V-statistics 
\begin_inset CommandInset citation
LatexCommand citet
key "serfling80"

\end_inset

.
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Formula $\tau$
\end_inset

-mixing is a notion of dependence within the observations, weak enough for
 most practical applications.
 Trivially, i.i.d.
 observations are 
\begin_inset Formula $\tau$
\end_inset

-mixing.
 As for the Markov Chains, whose convergance we study in the experiments,
 the property of geometric ergodicity implies 
\begin_inset Formula $\tau$
\end_inset

-mixing (given that the stationary distribution has a finite moment, see
 Appendix B of 
\begin_inset CommandInset citation
LatexCommand citet
key "chwialkowski2014kernel"

\end_inset

).
 For details on 
\begin_inset Formula $\tau$
\end_inset

-mixing, see 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
heiko{Kacper: put in tau mixing reference}
\end_layout

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
To formally introduce 
\begin_inset Formula $\tau$
\end_inset

-mixing, let 
\begin_inset Formula $\{Z_{t},\mathcal{G}_{t}\}_{t\in\mathbb{N}}$
\end_inset

 be a stationary sequence of integrable random variables with respect to
 a natural filtration 
\begin_inset Formula $\mathcal{G}_{t}$
\end_inset

.
 The sequence is called 
\begin_inset Formula $\tau$
\end_inset

-dependent if 
\begin_inset Formula $\tau(r)$
\end_inset

 converges to zero with 
\begin_inset Formula $r$
\end_inset

 going to infinity, where 
\begin_inset Formula $\tau$
\end_inset

 is defined as follows 
\begin_inset Formula 
\begin{align*}
\tau(r) & :=\sup_{l\in\mathbb{N}}\frac{1}{l}\sup_{r\leq i_{1}...\leq i_{l}}L(Z_{i_{1}},...,Z_{i_{l}})\quad\text{with}\\
L(Z) & :=\ev\left(\sup_{g\in\Lambda}\left|\int g(t)(dP_{Z|\mathcal{\mathcal{G}}_{0}}-dP)\right|\right),
\end{align*}

\end_inset

where 
\begin_inset Formula $\Lambda$
\end_inset

 is the set of all one-Lipschitz continuous real-valued functions on the
 domain of 
\begin_inset Formula $Z$
\end_inset

.
 For this work we will assume a technical condition 
\begin_inset Formula $\tau(t)\leq O(t^{-6})$
\end_inset

.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\lang english
A direct application of Theorem 2.1 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

 characterises the limiting behavior of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $nV_{n}$
\end_inset

 for 
\begin_inset Formula $\tau$
\end_inset

-mixing processes,
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\begin_layout Proposition

\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm: null_dist"

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Under the null hypothesis 
\begin_inset Formula $nV_{n}$
\end_inset

 converges weakly to some distribution.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
The proof, which is a simple verification of the assumptions, can be found
 in the Appendix.
 Although a formula for a limit distribution of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 can be derived explicitly (
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Theorem 2.1
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

), we do not provide it here.
 To our knowledge there are no methods of obtaining quantiles of a limit
 of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 in closed form.
 The common solution is to estimate quantiles by a resampling method, as
 described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-Kernel-Goodness-of-fit"

\end_inset

.
 The validity of this resampling method is guaranteed by the following propositi
on.
 
\end_layout

\begin_layout Proposition

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm:wild_bootstrap_works"

\end_inset

Let 
\begin_inset Formula $f(W_{1,n},\cdots,W_{t,n})=\sup_{x}|P(nB_{n}>x|W_{1,n},\cdots,W_{t,n})-P(nV_{n}>x)|$
\end_inset

 be a difference between quantiles.
 Under the null hypothesis, 
\begin_inset Formula $f(W_{1,n},\cdots,W_{t,n})$
\end_inset

 converges to zero in probability.
 Under the alternative hypothesis, 
\begin_inset Formula $B_{n}$
\end_inset

 converges to zero, while 
\begin_inset Formula $V_{n}$
\end_inset

 converges to a positive constant.
\end_layout

\begin_layout Standard
As a consequence, if the null hypothesis is true, we can approximate any
 quantile; while under the null hypothesis, all quantiles of 
\begin_inset Formula $B_{n}$
\end_inset

 collapse to zero while 
\begin_inset Formula $P(V_{n}>0)\to1$
\end_inset

.
\end_layout

\begin_layout Section

\lang english
Experiments
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sec:experiment"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We now provide a number of experimental illustrations of the usefullness
 of our test.
 After a simple sanity check on i.i.d.
 and non i.i.d.
 data, we give an illustrating of the testing bootstrap in the context of
 statistical model criticism for Gaussian Process (GP) regression.
 We then apply the proposed test to quantify bias-variance trade-offs of
 MCMC, and demonstrate how use the test to verify if the samples obtained
 form a MCMC sampling method can be assumed to be drawn from the stationary
 distribution.
 In the third experiment we
\lang british
 use the test for a different task -- convergence of a density estimator.
\end_layout

\begin_layout Subsubsection*

\lang english
Student's t vs Normal
\end_layout

\begin_layout Standard

\lang english
In this sanity check we modify the experiment 4.1 from 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

.
 The null hypothesis is that the observed samples come from a standard normal
 distribution.
 We study the power of the test against samples from Student's t distribution.
 We expect to observe low p-values when testing against a Student's t distributi
on with few degrees of freedom.
 1, 5, 10 or infinity are degrees of freedom that we consider in this experiment
, where infinity is equivalent to sampling from a standard normal distribution.
 For a fixed number of degrees of freedom we draw 1400 samples and calculate
 the p-value.
 This procedure is repeated one hundred times and the bar plot of p-values
 is plotted (see Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

).
 
\end_layout

\begin_layout Standard
The twist to 
\lang english
the experiment 4.1 by 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

 is that the draws from the Student's t distribution have temporal correlation.
 The samples are generated using Metropolis–Hastings algorithm, with a Gaussian
 random walk (variance equal to 0.5).
 We emphasize a need for an appropriate choice of the bootstrap process
 parameter, 
\begin_inset Formula $a_{n}$
\end_inset

, the probability of a sign flip.
 In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

 we plot p-values for 
\begin_inset Formula $a_{n}$
\end_inset

 being set to 
\begin_inset Formula $50\%$
\end_inset

.
 Such a high value of 
\begin_inset Formula $a_{n}$
\end_inset

 is 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
suitable for iid observations, but results in too conservative p-values
 for temporally correlated observations.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}=2\%$
\end_inset

, which gives well calibrated distribution of the p-values under the null
 hypothesis (see box plot for an infinite number degrees of freedom), however
 reduces the power of a the test.
 Indeed, p-values for five degrees of freedom are already large.
 The solution that we recommend is a mixture of thinning and adjusting 
\begin_inset Formula $a_{n},$
\end_inset

 as presented in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

.
 We have thinned the observations by a factor of 20, set 
\begin_inset Formula $a_{n}=10\%$
\end_inset

 and managed to preserve a good power and a right calibration of p-values
 under the null hypothesis.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang english
\begin_inset Graphics
	filename img/sgld_student_bad.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Large auto covariance, not a suitable bootstrap.
 The parameter 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}$
\end_inset

 is too large and the bootstrapped V-statistics 
\begin_inset Formula $B_{n}$
\end_inset

 are, on average, too low.
 Therefore it is very likely that 
\begin_inset Formula $V_{n}>B_{n}$
\end_inset

 and the test is too conservative.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:student_bad"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\lang english
\begin_inset Graphics
	filename img/sgld_student.pdf

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Large auto covariance and suitable bootstrap.
 The parameter 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}$
\end_inset

is chosen suitably, but due to a large auto-correlation withing the samples,
 the power of the test is small (effective sample size is small).
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\lang english
\begin_inset CommandInset label
LatexCommand label
name "fig:studentst"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename img/sgld_student_opt.pdf

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Thinned sample, and suitable bootstrap.
 Most of the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
auto-correlation within the sample is canceled by thinning.
 To guarantee that remaining auto-correlation is handled properly, the flip
 parameter is set 
\begin_inset Formula $10\%$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:thinning"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Statistical Model Criticism on Gaussian Processes
\end_layout

\begin_layout Standard
We now apply our test to the context of statistical model criticism on GP
 regression.
 Our presentation and approach is similar to the non i.i.d.
 case of 
\begin_inset CommandInset citation
LatexCommand citealt
after "Section 6"
key "lloyd2015statistical"

\end_inset

.
 We choose the Solar dataset consisting of a 1D regression problem with
 
\begin_inset Formula $N=402$
\end_inset

 pairs of 
\begin_inset Formula $(X,y)$
\end_inset

.
 We fit 
\begin_inset Formula $N_{\text{train}}=361$
\end_inset

 data fit using a GP using a standard exponentiated squared kernel and a
 Gaussian noise model to it, and perform standard maximum likelihood II
 on the hyperparameters (length-scale, overall scale, noise-variance).
 We then apply our test to the remaining 
\begin_inset Formula $N_{\text{test}}=41$
\end_inset

 data.
 Our test attempts to falsify the null hypothesis that the Solar dataset
 was generated from the predictive distribution (conditioned on training
 data and predicted position) of the GP.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "lloyd2015statistical"

\end_inset

 refer to this setup as non i.i.d.
 as the predictive distribution is a different univariate Gaussian for every
 predicted point.
 Note that in contrast to their MMD-based method, our test does 
\emph on
not
\emph default
 need to simulate from the multiple predictive distributions.
 Our particular 
\begin_inset Formula $N_{\text{train}},N_{\text{test}}$
\end_inset

 are chosen to make sure the GP fit has stabilised, i.e.
 adding more data does not cause further changes.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:experiment_gp"

\end_inset

 (top) shows training and testing data, and the fitted GP.
 Clearly, the Gaussian noise model is a poor fit for this particular dataset,
 e.g.
 around 
\begin_inset Formula $X=-1$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:experiment_gp"

\end_inset

 (bottom) shows the distribution over 
\begin_inset Formula $D=10000$
\end_inset

 bootstrapped V-statistics 
\begin_inset Formula $B_{n}$
\end_inset

 with 
\begin_inset Formula $n=N_{\text{test}}$
\end_inset

.
 Note that under the alternative distribution, the bootstrapped 
\begin_inset Formula $B_{n}$
\end_inset

 are not distributed according to the true null distribution.
 Their distribution, however, behaves favourbly to (correctly) rejecting
 the null hypothesis.
 Exacty this happens for this example: even for the low number of test data
 
\begin_inset Formula $N_{\text{test}}=41$
\end_inset

, our procedure rejects the hypothesis that the Solar dataset were generated
 from the fitted GP.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/gp_regression_data_fit.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/gp_regression_bootstrap_hist.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\series bold
Top
\series default
: Fitted GP and data used to fit (blue) and to apply test (red).
 
\series bold
Bottom
\series default
: Bootstrapped 
\begin_inset Formula $B_{n}$
\end_inset

 distribution with the test statistic 
\begin_inset Formula $V_{n}$
\end_inset

 marked.
 The test clearly rejects the hypothesis that the test points have been
 generated by the fitted GP.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:experiment_gp"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*

\lang english
Approximate MCMC algorithm 
\end_layout

\begin_layout Standard

\lang english
We show how to quantify
\lang british
 bias-variance trade-offs in an approximate 
\lang english
MCMC algorithm
\lang british
 -- 
\lang english
austerity MCMC 
\begin_inset CommandInset citation
LatexCommand citet
key "korattikara2013austerity"

\end_inset


\lang british
.
 
\lang english
For the purpose of illustration we use a simple generative from 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring,welling2011bayesian"

\end_inset

, 
\begin_inset Formula 
\begin{align*}
\theta_{1}\sim N(0,10);\theta_{2}\sim N(0,1)\\
X_{i}\sim\frac{1}{2}N(\theta_{1},4)+\frac{1}{2}N(\theta_{2},4) & .
\end{align*}

\end_inset

Austerity MCMC is a Monte Carlo procedure designed to reduce number of likelihoo
d evaluation in the acceptance step of the Metropolis-Hastings algorithm.
 The crux of method is to look just at a subset of the data and make a acceptanc
e/rejection decision based on this subset.
 The probability of making a wrong decision is proportional to parameter
 
\begin_inset Formula $\epsilon\in[0,1]$
\end_inset

 .
 Parameter 
\begin_inset Formula $\epsilon$
\end_inset

 influences time complexity of Austerity MCMC, the larger 
\begin_inset Formula $\epsilon$
\end_inset

, i.e.
 toleration for a mistake, the lower expected computational cost.
 We simulate 
\begin_inset Formula $\{X_{i}\}_{1\leq i\leq400}$
\end_inset

 points from the model with 
\begin_inset Formula $\theta_{1}=0$
\end_inset

 and 
\begin_inset Formula $\theta_{2}=1$
\end_inset

.
 In such a setting there are two modes in the posteriori distribution, one
 at the the point 
\begin_inset Formula $(0,1)$
\end_inset

 and the other at the point 
\begin_inset Formula $(1,-1)$
\end_inset

.
 We run the Austerity algorithm with 
\begin_inset Formula $\epsilon$
\end_inset

 varying in a range 
\begin_inset Formula $[0.001,0.2]$
\end_inset

.
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we calculate individual thinning factor such that correlation between consecuti
ve elements of samples form the chains is smaller than 
\begin_inset Formula $0.5$
\end_inset

 (greater 
\begin_inset Formula $\epsilon$
\end_inset

 require usually more less thinning).
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we then test a hypothesis that 
\begin_inset Formula $\{\theta_{i}\}_{1\leq i\leq500}$
\end_inset

 are drawn for a true stationary a posteriori distribution (using the developed
 test).
 This way we generate 100 p-values for each 
\begin_inset Formula $\epsilon$
\end_inset

 ,the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "p-values"

\end_inset

shows distribution of those p-values as a function of 
\begin_inset Formula $\epsilon$
\end_inset

.
 It is clear that 
\begin_inset Formula $\epsilon=0.09$
\end_inset

 yields a good approximation of true stationary distribution, while being
 parsimonious in terms of likelihood evaluations, as presented in the Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "lik-evals"

\end_inset

 (y-axis is in millions of evaluations).
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset CommandInset label
LatexCommand label
name "p-values"

\end_inset


\begin_inset Graphics
	filename img/Heiko1.pdf

\end_inset


\lang british

\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Distribution of p-values as a function of 
\begin_inset Formula $\epsilon$
\end_inset

 in the austerity MCMC.
 The null hypothesis is unlikely to be rejected for 
\begin_inset Formula $\epsilon=0.09$
\end_inset

, however it is only for 
\begin_inset Formula $\epsilon=0.001$
\end_inset

 that distribution of p-values resembles uniform distribution.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lik-evals"

\end_inset

 
\begin_inset Graphics
	filename img/Heiko2.pdf

\end_inset


\lang british

\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Average number of likelihood evaluations a function of 
\begin_inset Formula $\epsilon$
\end_inset

 in the austerity MCMC.
 Number of likelihood evaluations for 
\begin_inset Formula $\epsilon=0.09$
\end_inset

 is three times smaller then for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\epsilon=0.001$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Convergence in non-parametric density estimation
\end_layout

\begin_layout Standard
We now use the developed test to convergence in nonparametric density estimation.
 We quantify estimation quality and approximation quality of the infinite
 dimensional exponential family model 
\begin_inset CommandInset citation
LatexCommand citep
key "SriFukKumGreHyv14"

\end_inset

 and its recent random Fourier features approximation 
\begin_inset CommandInset citation
LatexCommand citep
key "strathmann2015gradient"

\end_inset

 respectively.
\end_layout

\begin_layout Standard
In this particular model log density is assumed to be of a form 
\begin_inset Formula $f(x)$
\end_inset

 where 
\begin_inset Formula $f$
\end_inset

 lies in a RKHS induced by a Gaussian kernel with bandwidth 
\begin_inset Formula $1$
\end_inset

.
 We fit the model to 
\begin_inset Formula $N$
\end_inset

 standard Gaussian distributed observations and perform our quadratic time
 test on separate evaluation dataset of a fixed size 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

.
 We aim to identify 
\begin_inset Formula $N$
\end_inset

 large enough (number of samples) so that the one-sample test does not reject
 hypothesis that assumed model of density is compatible with evaluation
 dataset.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_data"

\end_inset

 shows the distribution of p-values as a function of 
\begin_inset Formula $N$
\end_inset

; this distribution is uniform for 
\begin_inset Formula $N=5000$
\end_inset

, but already at 
\begin_inset Formula $N=500$
\end_inset

, the null hypothesis would very rarely be rejected.
\end_layout

\begin_layout Standard
We next use the recent random Features approximation 
\begin_inset CommandInset citation
LatexCommand citep
key "strathmann2015gradient"

\end_inset

 to the infinite dimensional exponential family model, where the log pdf,
 
\begin_inset Formula $f$
\end_inset

, is approximated using random Fourier features 
\begin_inset CommandInset citation
LatexCommand citep
key "Rahimi2007"

\end_inset

.
 The natural question when using this approximation is: 
\begin_inset Quotes eld
\end_inset

How many random features do it I need?
\begin_inset Quotes erd
\end_inset

 Using the same test power 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

 as above, and a large number of available samples 
\begin_inset Formula $N=5\cdot10^{4}$
\end_inset

, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_features"

\end_inset

 shows the distribution of p-values for an increasing number of random features
 
\begin_inset Formula $m$
\end_inset

.
 From about 
\begin_inset Formula $m=50$
\end_inset

, the null hypothesis would rarely be rejected for the chosen test power.
 Note that the p-values are not uniform, even for a large number of random
 features.
 This subtle effect is caused by the an over-smoothing due to the regularisation
 approach taken in 
\begin_inset CommandInset citation
LatexCommand citep
after "KMC finite"
key "strathmann2015gradient"

\end_inset

.
 It vanishes when the estimator is not regularised, however, at the cost
 of numerical instability.
 Our test sucessfully detected this behaviour.
\end_layout

\begin_layout Standard
 
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_data_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Density estimation: P-values for an increasing number of data 
\begin_inset Formula $N$
\end_inset

 for the non-parametric model.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_data"

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_features_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Aprpoximate density estimation: P-values for an increasing number of random
 features 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_features"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "icml2015"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Part*
Appendix
\end_layout

\begin_layout Section

\lang english
Proofs
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of 
\lang british
proposition
\lang english
 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset


\end_layout

\end_inset

The condition A1 is trivially stratified by the assumption 
\begin_inset Formula $\sum_{t=1}^{\infty}\sqrt{\tau(t)}\leq\infty$
\end_inset

.
 The condition A2 (iv), Lipschitz continuity.
 follows form the assumption iv).
 The positive definiteness and degeneracy follows form the proof of th the
 Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem_discrepancy_is_metric"

\end_inset

.
 Indeed
\end_layout

\begin_layout Proof

\lang english
\begin_inset Formula 
\[
h(x,y)=\langle\left[\nabla\log p(x)k(x,\cdot)+\nabla_{1}k(x,\cdot)\right],\left[\nabla\log p(y)k(y,\cdot)+\nabla_{1}k(y,\cdot)\right]\rangle_{\mathcal{F}^{d}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

so it's an inner product and hence positive definite.
 Degeneracy follows form the fact that for ant 
\begin_inset Formula $t$
\end_inset

, by Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

,
\begin_inset Formula $\ev\nabla\log p(x)k(x,t)+\nabla_{1}k(x,t)=0$
\end_inset

.
 Finally the condition A2 (iii), 
\begin_inset Formula $\ev h(X,X)\leq\infty$
\end_inset

 follows form (ii), (iii) and boundlessness of the kernel.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of 
\lang british
proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset


\end_layout

\end_inset

We have checked the assumption A2 in the proof of the Theorem 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset


\lang english
.
 The assumption B1 follows from our technical assumption 
\begin_inset Formula $\tau(t)\leq O(t^{-6})$
\end_inset

.
 Finally we check assumption B2 (bootstrap assumption):
\emph on
 
\begin_inset Formula $\{W_{t,n}\}_{1\leq t\leq n}$
\end_inset


\emph default
 is a row-wise strictly stationary triangular array independent of all 
\begin_inset Formula $Z_{t}$
\end_inset

 such that 
\begin_inset Formula $\ev W_{t,n}=0$
\end_inset

 and 
\begin_inset Formula $\sup_{n}\ev|W_{t,n}^{2+\sigma}|<\infty$
\end_inset

 for some 
\begin_inset Formula $\sigma>0$
\end_inset

.
 The auto-covariance of the process is given by 
\begin_inset Formula $\ev W_{s,n}W_{t,n}=(1-2p_{n})^{s-t}$
\end_inset

 , so the function 
\begin_inset Formula $\rho(x)=\exp(-x)$
\end_inset

, and 
\begin_inset Formula $l_{n}=\log(1-2p_{n})^{-1}$
\end_inset

.
 We verify that 
\begin_inset Formula $\lim_{u\to0}\rho(u)=1$
\end_inset

, 
\begin_inset Formula $l_{n}=o(n)$
\end_inset

 , 
\begin_inset Formula $\lim_{n\to\infty}l_{n}=\infty$
\end_inset

 and 
\begin_inset Formula $\sum_{r=1}^{n-1}\rho(|r|/l_{n})=O(l_{n})$
\end_inset

.
 
\end_layout

\begin_layout Subsection

\lang english
Linear time test
\end_layout

\begin_layout Standard

\lang english
For some fixed location 
\begin_inset Formula $y$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

, define a random variable 
\begin_inset Formula $s(X,y)$
\end_inset

 
\begin_inset Formula 
\begin{align}
s(X,y)=\nabla\log p(X)g(X,y)-\nabla g(X,y).
\end{align}

\end_inset

For some number of random locations 
\begin_inset Formula $Y_{1},Y_{J}$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

 define a random vector 
\begin_inset Formula $Z_{i}$
\end_inset

 
\begin_inset Formula 
\begin{equation}
Z_{i}=(s(X_{i},Y_{1}),\cdots,s(X_{i},Y_{J}))\in\mathbf{R}^{J}.
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\lang english
Let 
\begin_inset Formula $W_{n}$
\end_inset

 be a mean of 
\begin_inset Formula $Z_{i}$
\end_inset

's 
\begin_inset Formula $W_{n}=\frac{1}{n}\sum_{i=1}^{n}Z_{i},$
\end_inset

 and 
\begin_inset Formula $\Sigma_{n}$
\end_inset

 its covariance matrix 
\begin_inset Formula $\Sigma_{n}=\frac{1}{n}ZZ^{T}$
\end_inset

.
 The test statistic is 
\begin_inset Formula 
\begin{equation}
S_{n}=nW_{n}\Sigma_{n}^{-1}W_{n}.
\end{equation}

\end_inset

The computation of 
\begin_inset Formula $S_{n}$
\end_inset

 requires inversion of a 
\begin_inset Formula $J\times J$
\end_inset

 matrix 
\begin_inset Formula $\Sigma_{n}$
\end_inset

, but this is fast and numerically stable: 
\begin_inset Formula $J$
\end_inset

 will typically be small, and is less than 10 in our experiments.
 The next proposition demonstrates the use of 
\begin_inset Formula $S_{n}$
\end_inset

 as a one-sample test.
\end_layout

\begin_layout Proposition

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Asymptotic behavior of 
\begin_inset Formula $S_{n}$
\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "prop:Hotelling"

\end_inset

 If 
\begin_inset Formula $\ev s(X,y)=0$
\end_inset

 for all 
\begin_inset Formula $y$
\end_inset

, then the statistic 
\begin_inset Formula $S_{n}$
\end_inset

 is a.s.
 asymptotically distributed as a 
\begin_inset Formula $\chi^{2}$
\end_inset

-random variable with 
\begin_inset Formula $Jd$
\end_inset

 degrees of freedom, where 
\begin_inset Formula $d$
\end_inset

 is 
\begin_inset Formula $X$
\end_inset

 dimensionality (as 
\begin_inset Formula $n\to\infty$
\end_inset

 with 
\begin_inset Formula $d$
\end_inset

 fixed).
 If 
\begin_inset Formula $\ev s(X,y)\neq0$
\end_inset

 for almost all 
\begin_inset Formula $y$
\end_inset

 then a.s.
 for any fixed 
\begin_inset Formula $r$
\end_inset

, 
\begin_inset Formula $\mathbb{P}(S_{n}>r)\to1$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

 .
 
\end_layout

\begin_layout Paragraph

\lang english
One sample test
\end_layout

\begin_layout Standard

\lang english
Calculate 
\begin_inset Formula $S_{n}$
\end_inset

.
 Choose a threshold 
\begin_inset Formula $r_{\alpha}$
\end_inset

 corresponding to the 
\begin_inset Formula $1-\alpha$
\end_inset

 quantile of a 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution with 
\begin_inset Formula $J$
\end_inset

 degrees of freedom, and reject the null hypothesis whenever 
\begin_inset Formula $S_{n}$
\end_inset

 is larger than 
\begin_inset Formula $r_{\alpha}$
\end_inset

.
 
\end_layout

\end_body
\end_document
