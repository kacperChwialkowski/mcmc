#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\begin_preamble
% from the icml 2016 example tex file
\usepackage{times}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2016} 
\usepackage{placeins}

%\usepackage[accepted]{icml2016}


\newcommand{\heiko}[1]{   {\bf \color{blue}{HS: #1}}  }
\newcommand{\kacper}[1]{   {\bf \color{red}{K: #1}}  }
\newcommand{\arthur}[1]{   {\bf \color{magenta}{AG: #1}}  }

%\newcommand{\heiko}[1]{}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams
theorems-sec
\end_modules
\maintain_unincluded_children false
\language british
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type numerical
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[ 
\backslash
icmltitle{A Kernel Test of Goodness of Fit}
\end_layout

\begin_layout Plain Layout

% It is OKAY to include author information, even for blind
\end_layout

\begin_layout Plain Layout

% submissions: the style file will automatically remove it for you
\end_layout

\begin_layout Plain Layout

% unless you've provided the [accepted] option to the icml2015
\end_layout

\begin_layout Plain Layout

% package.
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Kacper Chwialkowski$^*$}{kacper.chwialkowski@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Heiko Strathmann$^*$}{heiko.strathmann@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmlauthor{Arthur Gretton}{arthur.gretton@gmail.com}
\end_layout

\begin_layout Plain Layout


\backslash
icmladdress{Gatsby Unit, University College London, United Kingdom}
\end_layout

\begin_layout Plain Layout

% You may provide any keywords that you 
\end_layout

\begin_layout Plain Layout

% find helpful for describing your paper; these are used to populate
\end_layout

\begin_layout Plain Layout

% the "keywords" metadata in the PDF but will not be shown in the document
\end_layout

\begin_layout Plain Layout


\backslash
icmlkeywords{kernel methods, goodness-of-fit, Stein's method, statistical
 testing}
\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Abstract
We propose a nonparametric statistical test for goodness-of-fit: given a
 set of samples, the test determines how likely it is that these were generated
 from a target density function.
 The measure of goodness-of-fit is a divergence constructed via Stein's
 method using functions from a Reproducing Kernel Hilbert Space.
 Our test statistic is based on an empirical estimate of this divergence,
 taking the form of a V-statistic in terms of the log gradients of the target
 density and the kernel.
 We derive a statistical test, both for i.i.d.
 and non-i.i.d.
 samples, where we estimate the null distribution quantiles using a wild
 bootstrap procedure.
 We apply our test to quantifying convergence of approximate Markov Chain
 Monte Carlo methods, statistical model criticism, and evaluating quality
 of fit vs model complexity in nonparametric density estimation.
\end_layout

\begin_layout Standard

\lang english
\begin_inset FormulaMacro
\newcommand{\ev}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Statistical tests of goodness-of-fit are a fundamental tool in statistical
 analysis, dating back to the test of Kolmogorov and Smirnov 
\begin_inset CommandInset citation
LatexCommand citep
key "Kolmogorov33,Smirnov48"

\end_inset

.
 Given a set of samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

 with distribution 
\begin_inset Formula $Z_{i}\sim q$
\end_inset

, our interest is in whether 
\begin_inset Formula $q$
\end_inset

 matches some reference or target distribution 
\begin_inset Formula $p$
\end_inset


\lang english
, which we assume to be only known up to the normalisation constant.
 Recently, in the multivariate setting, 
\lang british

\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring"

\end_inset

 proposed an elegant measure of sample quality with respect to a target.
 This measure is a maximum discrepancy between empirical sample expectations
 and target expectations over a large class of test functions, constructed
 so as to have zero expectation over the target distribution by use of a
 Stein operator.
 This operator depends only on the derivative of the 
\begin_inset Formula $\log q$
\end_inset

: thus, the approach can be applied very generally, as it does not require
 closed-form integrals over the target distribution (or numerical approximations
 of such integrals).
 By contrast, many earlier discrepancy measures require integrals with respect
 to the target (see below for a review).
 This is problematic if the intention is to perform benchmarks for assessing
 Markov Chain Monte Carlo, since these integrals will certainly not be known
 to the practitioner.
\end_layout

\begin_layout Standard
A challenge in applying the approach of 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "gorham2015measuring"

\end_inset

 is the complexity of the function class used, which results from applying
 the Stein operator to the bounded Lipschitz functions.
\begin_inset Foot
status open

\begin_layout Plain Layout
The bounded Lipschitz functions give rise to the Wasserstein integral probabilit
y metric.
 By contrast, the Kolmogorov-Smirnov test uses functions of bounded variation
 1 
\begin_inset CommandInset citation
LatexCommand citep
key "Mueller97"

\end_inset

.
 Multivariate generalisations of the K-S test exist, however the computational
 cost of a consistent test rapidly becomes prohibitive with increasing dimension
 
\begin_inset CommandInset citation
LatexCommand citep
key "Justel1997251"

\end_inset

.
\end_layout

\end_inset

 Thus, their sample quality measure requires solving an expensive linear
 program that arises from a complicated construction of graph Stein discrepancie
s and geometric spanners.
 Their metric furthermore requires access to nontrivial lower bounds that,
 despite being provided for log-concave densities, are a largely open problem
 otherwise, in particular for multivariate cases.
\end_layout

\begin_layout Standard
An important application of a goodness-of-fit measure is in statistical
 testing, where it is desired to determine whether the empirical discrepancy
 measure is large enough to reject the null hypothesis (that the sample
 arises from the target distribution).
 One approach is to establish the asymptotic behaviour of the test statistic,
 and to set a test threshold at a large quantile of the asymptotic distribution.
 The asymptotic behaviour of the Wasserstein-based Stein discrepancy remains
 a challenging open problem, due to the complexity of the function class
 used.
 It is not clear how one would compute p-values for this statistic, or determine
 when the goodness of fit test would allow us to accept the null hypothesis
 (at the user-specified test level).
\end_layout

\begin_layout Standard
The key contribution of this work is to define a statistical test of goodness-of
-fit, based on a Stein discrepancy computed in a Reproducing Kernel Hilbert
 Space (RKHS).
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
%AG
\end_layout

\end_inset

 To construct our test statistic, we use a function class defined by applying
 the Stein operator to a chosen space of RKHS functions, as proposed by
 
\begin_inset CommandInset citation
LatexCommand citep
key "OatGirCho15"

\end_inset

.
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citeauthor
key "OatGirCho15"

\end_inset

 addressed the problem variance reduction in Monte Carlo integration, using
 the Stein operator to avoid bias.
 
\end_layout

\end_inset

 Our measure of goodness of fit is the largest discrepancy over this space
 of functions between empirical sample expectations and target expectations
 (the latter being zero, due to the effect of the Stein operator).
 The approach is a natural extension to goodness-of-fit testing of the earlier
 two-sample tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton2012kernel"

\end_inset

 and independence tests 
\begin_inset CommandInset citation
LatexCommand citep
key "gretton_kernel_2008"

\end_inset

 based on the maximum mean discrepancy, which is an integral probability
 metric.
 As with these earlier tests, our statistic is a simple V-statistic, and
 can be computed in closed form and in quadratic time; moreover, it is an
 unbiased estimate of the corresponding population discrepancy.
 As with all Stein-based discrepancies, only the gradient of the log-density
 of the target density is needed; we do not require integrals with respect
 to the target density -- including the normalisation constant.
 Given that our test statistic is a V-statistic, we may make use of the
 extensive literature on asymptotics of V-statistics to formulate a hypothesis
 test 
\begin_inset CommandInset citation
LatexCommand citep
key "serfling80,leucht_dependent_2013"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
%AG
\end_layout

\end_inset

 We are able to provide statistical tests for both uncorrelated and correlated
 samples, where the latter is essential if the test is to be used in assessing
 the quality of output of an MCMC procedure.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
%AG
\end_layout

\end_inset

 An identical test was obtained simultaneously in independent work by 
\begin_inset CommandInset citation
LatexCommand citet
key "LiuLeeJor16"

\end_inset

, for uncorrelated samples.
\end_layout

\begin_layout Standard
Several alternative approaches exist in the statistics literature to goodness-of
-fit testing.
 A first strategy is to partition the space, and to conduct the test on
 a histogram estimate of the distribution 
\begin_inset CommandInset citation
LatexCommand citep
key "Bar89,Beirlant2,Gyorfi,GyVa02"

\end_inset

.

\lang english
 Such space partitioning approaches can have attractive theoretical properties
 (e.g.
 distribution-free test thresholds) and work well in low dimensions, however
 they are much less powerful than alternatives once the dimensionality increases
 
\begin_inset CommandInset citation
LatexCommand cite
key "GreGyo10"

\end_inset

.

\lang british
 A second popular approach has been to use the smoothed 
\begin_inset Formula $L_{2}$
\end_inset

 distance between the empirical characteristic function of the sample, and
 the characteristic function of the target density.
 This dates back to the test of Gaussianity of 
\begin_inset CommandInset citation
LatexCommand citet
key "BaringhausHenze88"

\end_inset

, who used a squared exponential smoothing function (see Eq.
 2.1 in their paper).
 For this choice of smoothing function, their statistic is identical to
 the maximum mean discrepancy (MMD) with the squared exponential kernel,
 which can be shown using the Bochner representation of the kernel (compare
 with 
\begin_inset CommandInset citation
LatexCommand citealt
after "Corollary 4"
key "SriGreFukLanetal10"

\end_inset

).
 It is essential in this case that the target distribution be Gaussian,
 since the convolution with the kernel (or in the Fourier domain, the smoothing
 function) must be available in closed form.
 An 
\begin_inset Formula $L_{2}$
\end_inset

 distance between Parzen window estimates can also be used  
\begin_inset CommandInset citation
LatexCommand citep
key "BowFos93"

\end_inset

, giving the same expression again, although the optimal choice of bandwidth
 for consistent Parzen window estimates may not be a good choice for testing
 
\begin_inset CommandInset citation
LatexCommand citep
key "AndHalTit94"

\end_inset

.
 A different smoothing scheme in the frequency domain results in an energy
 distance statistic 
\begin_inset CommandInset citation
LatexCommand citep
before "this likewise being an MMD with a particular choice of kernel; see "
key "SejSriGreFuk13"

\end_inset

, which can be used in a test of normality 
\begin_inset CommandInset citation
LatexCommand citep
key "SzeRiz05"

\end_inset

.
 The key point is that the required integrals are again computable in closed
 form for the Gaussian, although the reasoning may be extended to certain
 other families of interest, e.g.
 
\begin_inset CommandInset citation
LatexCommand citep
key "Rizzo09"

\end_inset

.
 The requirement of computing closed-form integrals with respect to the
 test distribution severely restricts this testing strategy.
 Finally, a problem related to goodness-of-fit testing is that of model
 criticism 
\begin_inset CommandInset citation
LatexCommand citep
key "lloyd2015statistical"

\end_inset

.
 In this setting, samples generated from a fitted model are compared via
 the maximum mean discrepancy with samples used to train the model, such
 that a small MMD indicates a good fit.
 There are two limitation to the method: first, it requires samples from
 the model (which might not be easy if this requires a complex MCMC sampler);
 second, the choice of number of samples from the model is not obvious,
 since too few samples cause a loss in test power, and too many are computationa
lly wasteful.
 Neither issue arises in our test, since we do not require model samples.
\end_layout

\begin_layout Standard
In our experiments, a particular focus is on applying our goodness-of-fit
 test to certify the output of approximate Markov Chain Monte Carlo (MCMC)
 samplers 
\begin_inset CommandInset citation
LatexCommand citep
key "Korattikara2014,Welling2011,Bardenet2014"

\end_inset

.
 These methods use modifications to Markov transition kernels that improve
 mixing speed at the cost of worsening the asymptotic bias.
 The bias-variance trade-off can usually be tuned with parameters of the
 sampling algorithms.
 It is therefore important to test whether for a particular parameter setting
 and run-time, the samples are of the desired quality.
 This question cannot be answered with classical MCMC convergence statistics,
 such as the widely used potential scale reduction factor (R-factor) 
\begin_inset CommandInset citation
LatexCommand citep
key "gelman1992inference"

\end_inset

 or the effective sample size, since these assume that the Markov chain
 reaches its equilibrium distribution.
 By contrast, our test exactly quantifies the asymptotic bias of approximate
 MCMC.
 
\end_layout

\begin_layout Paragraph
Paper outline
\end_layout

\begin_layout Standard
We begin our presentation in the section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-Kernel-Goodness-of-fit"

\end_inset

 with a high-level construction of the RKHS-based Stein discrepancy and
 associated statistical test.
 In Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

, we provide additional details and prove the main results.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:experiment"

\end_inset

 contains experimental illustrations on synthetic examples, statistical
 model criticism, bias-variance trade-offs in approximate MCMC, and convergence
 in non-parametric density estimation.
\end_layout

\begin_layout Section

\lang english
Test Definition: Statistic and Threshold
\begin_inset CommandInset label
LatexCommand label
name "sec:A-Kernel-Goodness-of-fit"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We begin with a high-level construction of our divergence discrepancy and
 the statistical test.
 While this section aims to communicate the main ideas, we provide details
 and proofs in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

.
\end_layout

\begin_layout Subsection

\lang english
Stein Operator in RKHS
\end_layout

\begin_layout Standard

\lang english
Our goal is to write the maximum discrepancy between target distribution
 
\begin_inset Formula $p$
\end_inset

 and observed sample distribution 
\begin_inset Formula $q$
\end_inset

 in a RKHS.
 Denote by 
\begin_inset Formula ${\cal F}$
\end_inset

 the RKHS of real-valued functions
\begin_inset Note Note
status open

\begin_layout Plain Layout
%AG: added 
\backslash
Re^d condition
\end_layout

\end_inset

 on 
\begin_inset Formula $\mathbb{R}^{d}$
\end_inset

 with reproducing kernel 
\begin_inset Formula $k$
\end_inset

, and by 
\begin_inset Formula ${\cal F}^{d}$
\end_inset

 the product RKHS consisting of elements 
\begin_inset Formula $f:=(f_{1},\dots,f_{d})$
\end_inset

 with 
\begin_inset Formula $f_{i}\in{\cal F}$
\end_inset

, and with a standard inner product 
\begin_inset Formula $\left\langle f,g\right\rangle _{\mathcal{F}^{d}}=\sum_{i=1}^{d}\left\langle f_{i},g_{i}\right\rangle _{\mathcal{F}}$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\lang english
%AG: added the inner product.
 Added new citation to Oates.
\end_layout

\end_inset

 Similarly to 
\begin_inset CommandInset citation
LatexCommand citet
key "stein1972,gorham2015measuring,OatGirCho15"

\end_inset

, we begin by defining a Stein operator 
\begin_inset Formula $T$
\end_inset

 acting on 
\begin_inset Formula $f\in\mathcal{F}^{d}$
\end_inset

 
\begin_inset Formula 
\[
Tf:=\sum_{i=1}^{d}\left(\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right).
\]

\end_inset

Suppose a random variable 
\begin_inset Formula $Z$
\end_inset

 is distributed according to a measure
\begin_inset Foot
status open

\begin_layout Plain Layout
Throughout the article, all occurrences of 
\begin_inset Formula $Z$
\end_inset

, e.g.
 
\begin_inset Formula $Z',Z_{i},Z_{\heartsuit}$
\end_inset

, are understood to be distributed according to 
\begin_inset Formula $q$
\end_inset

.
\end_layout

\end_inset

 
\begin_inset Formula $q$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

 is distributed according to the target measure 
\begin_inset Formula $p$
\end_inset

.
 As we will see, the operator can be expressed by defining a function that
 depends on gradients of the log-density and the kernel, 
\begin_inset Formula 
\begin{equation}
\xi(x,\cdot):=\left[\nabla\log p(x)k(x,\cdot)+\nabla k(x,\cdot)\right],\label{eq:xi}
\end{equation}

\end_inset

whose inner product with 
\begin_inset Formula $f$
\end_inset

 gives exactly the expected value of the Stein operator 
\begin_inset Formula 
\[
\ev Tf(Z)=\langle f,\ev\xi(Z)\rangle_{{\cal F}^{d}}=\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(Z)\rangle_{{\cal F}},
\]

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

.
 For 
\begin_inset Formula $X$
\end_inset

 from the target measure, we have 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

, which can be seen using integration by parts, c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

 in the supplement.
 We can now define a Stein discrepancy and express it in the RKHS,
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscripts to dot products.
\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
S(Z) & :=\sup_{\Vert f\Vert<1}\ev(Tf)(Z)-\ev(Tf)(X)\\
 & =\sup_{\Vert f\Vert<1}\langle f,\ev\xi(Z)-\ev\xi(X)\rangle_{{\cal F}^{d}}\\
 & =\sup_{\Vert f\Vert<1}\langle f,\ev\xi(Z)\rangle_{{\cal F}^{d}}\\
 & =\|\ev\xi(Z)\|_{{\cal F}^{d}},
\end{align*}

\end_inset

c.f.
 Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:discprepancy_maximised_by_norm"

\end_inset

.
 This makes it clear why 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

 is a desirable property: we can compute 
\begin_inset Formula $S(Z)$
\end_inset

 by computing 
\begin_inset Formula $\|\ev\xi(Z)\|$
\end_inset

, without the need to access 
\begin_inset Formula $X$
\end_inset

 in the form of samples from 
\begin_inset Formula $p$
\end_inset

.
 We arrive at our first main result, which states that the above discrepancy
 can be used to distinguish two distributions 
\begin_inset Formula $p,q\in{\cal P}$
\end_inset

.
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "theorem_discrepancy_is_metric"

\end_inset

 Let 
\begin_inset Formula $q,p\in\mathcal{P}$
\end_inset

 where the derivatives of elements of 
\begin_inset Formula $\mathcal{P}$
\end_inset

 satisfy assumption (ii) in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

, and let 
\begin_inset Formula $Z\sim q$
\end_inset

.
 Let the RKHS 
\begin_inset Formula $\mathcal{F}$
\end_inset

 satisfy properties (iii) and (v) in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

, which include the requirement that 
\begin_inset Formula $\mathcal{F}$
\end_inset

 be cc-universal 
\begin_inset CommandInset citation
LatexCommand citep
after "Definition 4.1"
key "carmeli2010vector"

\end_inset

.
 Then 
\begin_inset Formula $S(Z)=0$
\end_inset

 if and only if 
\begin_inset Formula $p=q$
\end_inset

.
 
\end_layout

\begin_layout Standard
Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:details_kernel_stein"

\end_inset

 contains the formal statements of the assumptions on 
\begin_inset Formula ${\cal P}$
\end_inset

 and 
\begin_inset Formula $\mathcal{F}$
\end_inset

, and a proof.
 The following theorem gives a simple closed form expression.
\end_layout

\begin_layout Theorem

\lang english
\begin_inset CommandInset label
LatexCommand label
name "th:closed_form_discrepancy"

\end_inset

 Let
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscript to dot products.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{align*}
h(x,y) & :=\nabla\log p(x)^{\top}\nabla\log p(y)k(x,y)\\
 & \quad+\nabla\log p(y)^{\top}\nabla_{x}k(x,y)\\
 & \quad+\nabla\log p(x){}^{\top}\nabla_{y}k(x,y)\\
 & \quad+\langle\nabla_{x}k(x,\cdot),\nabla_{y}k(\cdot,y)\rangle_{{\cal F}^{d}},
\end{align*}

\end_inset


\end_layout

\begin_layout Theorem

\lang english
where the last term can be written as a sum 
\begin_inset Formula $\sum_{\{i=1\}}^{d}\frac{\partial k(x,y)}{\partial x_{i}\partial y_{i}}$
\end_inset

.
 The 
\lang british
squared Stein discrepancy is
\lang english
 
\begin_inset Formula $S(Z)^{2}=\ev h(Z,Z')$
\end_inset

.

\lang british
 
\end_layout

\begin_layout Standard
We now proceed with constructing an estimator for 
\begin_inset Formula $S(Z)^{2}$
\end_inset

, and outline its asymptotic properties.
\end_layout

\begin_layout Subsection
Wild Bootstrap Testing
\end_layout

\begin_layout Standard
It is straightforward to estimate the squared Stein discrepancy 
\begin_inset Formula $S(Z)^{2}$
\end_inset

 from samples 
\begin_inset Formula $\{Z_{i}\}_{i=1}^{n}$
\end_inset

: a quadratic time estimator is a V-Statistic, and takes the form
\begin_inset Formula 
\[
V_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}h(Z_{i},Z_{j}).
\]

\end_inset

 The asymptotic null distribution of the normalised V-Statistic 
\begin_inset Formula $nV_{n}$
\end_inset

, however, has no computable closed form.
 Furthermore, care has to be taken when the 
\begin_inset Formula $Z_{i}$
\end_inset

 exhibit correlation structure, as the null distribution significantly changes,
 impacting test significance.
 The wild bootstrap technique 
\begin_inset CommandInset citation
LatexCommand citep
key "Shao2010,leucht_dependent_2013,FroLauLerRey12"

\end_inset

 addresses both problems.
 First, it allows to simulate from the null distribution to compute test
 thresholds.
 Second, it accounts for correlation structure in the 
\begin_inset Formula $Z_{i}$
\end_inset

 by mimicking it with an 
\lang english
auxiliary
\lang british
 random process: a
\lang english
 Markov chain taking values in 
\begin_inset Formula $\{-1,1\}$
\end_inset

, starting from 
\begin_inset Formula $W_{1,n}=1$
\end_inset

,
\lang british

\begin_inset Formula 
\[
W_{t,n}=\mathbf{1}(U_{t}>a_{n})W_{t-1,n}-\mathbf{1}(U_{t}<a_{n})W_{t-1,n},
\]

\end_inset


\lang english
where the 
\begin_inset Formula $U_{t}$
\end_inset

 are uniform i.i.d.
 random variables and 
\begin_inset Formula $a_{n}$
\end_inset

 is the probability of 
\begin_inset Formula $W_{t,n}$
\end_inset

 changing sign (for i.i.d.
 data we may set 
\begin_inset Formula $a_{n}=0.5$
\end_inset

).
 This leads to a bootstrapped V-statistic 
\end_layout

\begin_layout Standard

\emph on
\lang english
\begin_inset Formula 
\[
B_{n}=\frac{1}{n^{2}}\sum_{i,j=1}^{n}W_{i,n}W_{j,n}h(Z_{i,}Z_{j}).
\]

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 establishes that, under the null hypothesis, 
\begin_inset Formula $nB_{n}$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
 is a good approximation
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
of 
\begin_inset Formula $nV_{n}$
\end_inset

, so it is possible to approximate quantiles of the null distribution by
 sampling from it.
 Under the alternative, however, 
\begin_inset Formula $V_{n}$
\end_inset

 dominates 
\begin_inset Formula $B_{n}$
\end_inset

 -- resulting in almost sure rejection of the null hypothesis.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
We propose the following test
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 procedure for testing the null hypothesis that the 
\begin_inset Formula $Z_{i}$
\end_inset

 are distributed according to the target distribution 
\begin_inset Formula $p$
\end_inset

.
\end_layout

\begin_layout Itemize
Calculate 
\lang english
the test statistic 
\begin_inset Formula $V_{n}$
\end_inset

.
\end_layout

\begin_layout Itemize

\lang english
Obtain wild bootstrap samples
\lang british
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\{B_{n}\}_{i=1}^{D}$
\end_inset

 and estimate the 
\begin_inset Formula $1-\alpha$
\end_inset

 empirical quantile of these samples.
 
\end_layout

\begin_layout Itemize

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
If 
\begin_inset Formula $V_{n}$
\end_inset

 exceeds the quantile, reject.
\end_layout

\begin_layout Section

\lang english
Proofs of the Main Results
\begin_inset CommandInset label
LatexCommand label
name "sec:Details"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We now prove the claims made in the previous Section.
\end_layout

\begin_layout Subsection

\lang english
Stein Operator in RKHS
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sec:details_kernel_stein"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We make the following assumptions.
 Let 
\begin_inset Formula ${\cal P}$
\end_inset

 be a family of distributions on a real coordinate space, where its elements
 
\begin_inset Formula $p\in\mathcal{P}$
\end_inset

 satisfy two conditions:
\end_layout

\begin_layout Standard

\lang english
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{itemize}
\end_layout

\begin_layout Plain Layout

	
\backslash
item[(i)]  $
\backslash
nabla 
\backslash
log p(x)$ is Lipschitz continuous.
\end_layout

\begin_layout Plain Layout

	
\backslash
item[(ii)] $
\backslash
mathbb E 
\backslash
| 
\backslash
nabla 
\backslash
log p(Z) 
\backslash
|^2 
\backslash
leq 
\backslash
infty$ for any random variable.
\end_layout

\begin_layout Plain Layout


\backslash
end{itemize}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
The kernels 
\begin_inset Formula $k$
\end_inset

 considered in this work satisfy
\end_layout

\begin_layout Standard

\lang english
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{itemize}
\end_layout

\begin_layout Plain Layout

	
\backslash
item[(iii)]  $
\backslash
mathbb E  
\backslash
left( 
\backslash
frac{
\backslash
partial^2 k(Z,Z)}{
\backslash
partial x_i 
\backslash
partial x_{i+d}} 
\backslash
right)^2 
\backslash
leq 
\backslash
infty$.
 
\end_layout

\begin_layout Plain Layout

	
\backslash
item[(iv)] $
\backslash
nabla_x k(x,y)$ is Lipschitz continuous.
\end_layout

\begin_layout Plain Layout

    
\backslash
item[(v)] $k$ is bounded, symmetric and cc-universal 
\backslash
citep{carmeli2010vector}.
\end_layout

\begin_layout Plain Layout


\backslash
end{itemize}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Requirements (i) and (iv) are used in Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset

 regarding the wild-bootstrap procedure, (ii) and (iii) are needed for Bocher
 integrability of 
\begin_inset Formula $\xi$
\end_inset

 in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt1"

\end_inset

, and (v) is needed in the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset

.

\lang english
 
\end_layout

\begin_layout Standard

\lang english
We show in Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

 in the Appendix that the expected value of the Stein operator is zero on
 the target measure.
\end_layout

\begin_layout Standard

\lang english
The following lemmas are useful in proving our main results, Theorems 
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:WellDefined"

\end_inset

 
\begin_inset Formula $\xi(x,\cdot)$
\end_inset

 (see 
\lang british
Eq.
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:xi"

\end_inset


\lang english
) is an element of the reproducing kernel Hilbert space 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

.
 
\end_layout

\begin_layout Proof

\lang english
We use the proof of 
\begin_inset CommandInset citation
LatexCommand citet
after "Corollary 4.36"
key "SteChr08"

\end_inset

 to see that for all 
\begin_inset Formula $x\in R^{d}$
\end_inset

 each entry of 
\begin_inset Formula $\nabla k(x,\cdot)$
\end_inset

 belongs to 
\begin_inset Formula $\mathcal{F}$
\end_inset

.
 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}k(x,\cdot)\in\mathcal{F}$
\end_inset

, since 
\begin_inset Formula $k(x,\cdot)\in\mathcal{F}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial\log p(x)}{\partial x_{i}}$
\end_inset

 is a scalar.
 
\end_layout

\begin_layout Standard
The following lemma shows that the expected value of 
\begin_inset Formula $\xi$
\end_inset

 is well defined -- it is needed for establishing a link between Stein operator
 
\begin_inset Formula $Tf$
\end_inset

 and 
\begin_inset Formula $\xi$
\end_inset

.
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:BochnerInt1"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, the expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

 is an element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

 (
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\xi$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 is Bochner integrable wrt the measure of 
\begin_inset Formula $Z$
\end_inset

).
 
\end_layout

\begin_layout Proof

\lang english
It is sufficient to check that coefficients of 
\begin_inset Formula $\xi$
\end_inset

 are Bochner integrable 
\begin_inset CommandInset citation
LatexCommand cite
after "Definition A.5.20"
key "SteChr08"

\end_inset

.
 First we check that for any random variable 
\begin_inset Formula $Z$
\end_inset

,
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscript to norm.
\end_layout

\end_inset

 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)\right\Vert _{\mathcal{F}}^{2}<C\ev\|\nabla\log p(X)\|^{2}<\infty,
\]

\end_inset

for some constant 
\begin_inset Formula $C$
\end_inset

, which follows from assumption (ii) and boundedness of the kernel.
 Next we check that
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscript to norm.
\end_layout

\end_inset

 
\begin_inset Formula 
\[
\ev\left\Vert \frac{\partial k(Z,\cdot)}{\partial x}\right\Vert _{\mathcal{F}^{d}}^{2}=\ev\left(\frac{\partial^{2}k(Z,Z)}{dx_{i}dx_{i+d}}\right)^{2}<\infty,
\]

\end_inset

which follows from assumption (iii).
\end_layout

\begin_layout Standard
We can now show that the expected value of the Stein operator can be expressed
 as an inner product with an element of 
\begin_inset Formula $\mathcal{F}^{d}$
\end_inset

, where this element is the expected value of 
\begin_inset Formula $\xi$
\end_inset

.
 
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:SteinIsInner"

\end_inset

For any random variable 
\begin_inset Formula $Z$
\end_inset

, the expected value of the Stein operator coincides with the inner product
 of 
\begin_inset Formula $f$
\end_inset

 and the expected value of 
\begin_inset Formula $\xi(Z)$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
\ev Tf(Z)=\langle f,\ev\xi(Z)\rangle_{\mathcal{F}^{d}} & =\sum_{i=1}^{d}\langle f_{i},\ev\xi_{i}(Z)\rangle_{\mathcal{F}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Proof

\lang english
We write
\begin_inset Formula 
\begin{align*}
 & \left\langle f_{i},\ev\xi_{i}(Z)\right\rangle _{\mathcal{F}}\\
 & =\left\langle f_{i},\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right]\right\rangle _{\mathcal{F}}\\
 & =\ev\left\langle f_{i},\frac{\partial\log p(Z)}{\partial x_{i}}k(Z,\cdot)+\frac{\partial k(Z,\cdot)}{\partial x_{i}}\right\rangle _{\mathcal{F}}\\
 & =\ev\left[\frac{\partial\log p(Z)}{\partial x_{i}}f_{i}(Z)+\frac{\partial f(Z,\cdot)}{\partial x_{i}}\right].
\end{align*}

\end_inset

The second equality follows from the fact that a linear operator 
\begin_inset Formula $\langle f_{i},\cdot\rangle_{\mathcal{F}}$
\end_inset

 can be interchanged with the Bochner integral, and the fact that 
\begin_inset Formula $\xi$
\end_inset

 is Bochner integrable (Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:BochnerInt1"

\end_inset

).
 The last equality is an application of the reproducing property.
 
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
From the inner product representation, we get  
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:discprepancy_maximised_by_norm"

\end_inset

The discrepancy 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximized by the expected value of 
\begin_inset Formula $\xi$
\end_inset

, i.e.
 
\begin_inset Formula $S(Y,\mathcal{F},p)=\|\ev\xi(Y)\|$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
By the Lemma 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "lem:SteinIsInner"

\end_inset

, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $\ev Tf(Y)=\langle f,\ev\xi(Y)\rangle_{\mathcal{F}^{d}}$
\end_inset

 and therefore, 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is maximized by 
\begin_inset Formula $\frac{\ev\xi(Y)}{\|\ev\xi(Y)\|_{\mathcal{F}^{d}}}$
\end_inset

.
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscript to norms.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
We are now ready for the proof
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british
 of the closed form formula for 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english

\begin_inset Formula $S(Y,\mathcal{F},p)^{2}$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "th:closed_form_discrepancy"

\end_inset


\end_layout

\end_inset

 We use the notation 
\begin_inset Formula 
\begin{align*}
\nabla_{x}k(x,\cdot)=\left(\frac{\partial k(x,\cdot)}{\partial x_{1}},\cdots,\frac{\partial k(x,\cdot)}{\partial x_{d}}\right)\\
\nabla_{y}k(\cdot,y)=\left(\frac{\partial k(\cdot,y)}{\partial y_{1}},\cdots,\frac{\partial k(\cdot,y)}{\partial y_{d}}\right),
\end{align*}

\end_inset

giving
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
%AG: added subscript to norms.
\end_layout

\end_inset

 
\begin_inset Formula 
\begin{align*}
 & S(X,\mathcal{F},p)^{2}=\langle\ev\xi,\ev\xi\rangle_{\mathcal{F}^{d}}\\
 & =\langle\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X,\cdot)\right],\\
 & \quad\quad\ev\left[\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X,\cdot)\right]\rangle_{\mathcal{F}^{d}}\\
 & =\ev\langle\nabla\log p(X)k(X,\cdot)+\nabla_{x}k(X,\cdot),\\
 & \quad\quad\quad\nabla\log p(X)k(\cdot,X')+\nabla_{y}k(\cdot,X')\rangle_{\mathcal{F}^{d}}\\
 & =\ev\nabla\log p(X)^{\top}\nabla\log p(X')k(X,X')\\
 & \quad+\ev\nabla\log p(X)\nabla_{x}k(X,X')\\
 & \quad+\ev\nabla\log p(X){}^{\top}\nabla_{y}k(X,X')\\
 & \quad+\ev\langle\nabla_{x}k(X,\cdot),\nabla_{y}k(\cdot,X')\rangle_{\mathcal{F}^{d}}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Finally, we prove that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
the discrepancy 
\begin_inset Formula $S$
\end_inset

 discriminates different probability measures.
 
\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "theorem_discrepancy_is_metric"

\end_inset


\end_layout

\end_inset

 If 
\begin_inset Formula $p=q$
\end_inset

 then 
\begin_inset Formula $S(Y,\mathcal{F},p)$
\end_inset

 is 
\begin_inset Formula $0$
\end_inset

 by Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

.
 Suppose 
\begin_inset Formula $p\neq q$
\end_inset

, but 
\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

.
 If 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $S(Y,\mathcal{F},p)=0$
\end_inset

 then 
\begin_inset Formula $\ev\xi(Y)=0.$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 For each dimension of 
\begin_inset Formula $\ev\xi(Y)$
\end_inset

, we add and subtract 
\begin_inset Formula $\log q(Y)$
\end_inset

, 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial}{\partial x_{i}}\log p(Y)k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & =\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)\\
 & \quad+\ev\left(\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)\right).
\end{align*}

\end_inset

We have used Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:easy"

\end_inset

 to see that 
\begin_inset Formula 
\[
\ev\left(\frac{\partial}{\partial x_{i}}(\log q(Y))k(Y,\cdot)+\frac{\partial}{\partial x_{i}}k(Y,\cdot)\right)=0.
\]

\end_inset

We 
\lang british
recognise
\lang english
 that the expected value of 
\begin_inset Formula $\frac{\partial}{\partial x_{i}}(\log p(Y)-\log q(Y))k(Y,\cdot)$
\end_inset

 is the mean embedding of a function 
\begin_inset Formula $g(y)=\frac{\partial}{\partial x_{i}}\left(\log\frac{p(y)}{q(y)}\right)$
\end_inset

 with respect to the measure 
\begin_inset Formula $q$
\end_inset

.
 By assumption (ii) function 
\begin_inset Formula $g$
\end_inset

 is square integrable, 
\begin_inset Formula $\ev(\frac{\partial}{\partial x_{i}}\log p(Y))^{2}\leq\infty$
\end_inset

 and 
\begin_inset Formula $\ev(\frac{\partial}{\partial x_{i}}\log q(Y))^{2}\leq\infty$
\end_inset

 .
 Therefore, since the kernel 
\begin_inset Formula $k$
\end_inset

 is cc-universal, by 
\begin_inset CommandInset citation
LatexCommand citet
after " Theorem 4.4 c"
key "carmeli2010vector"

\end_inset

 this embedding is zero if and only if 
\begin_inset Formula $g=0$
\end_inset

, which implies that 
\begin_inset Formula 
\[
\nabla\log\frac{p(y)}{q(y)}=(0,\cdots,0).
\]

\end_inset

A constant vector field of derivatives can only be generated by a constant
 function, so 
\begin_inset Formula $\log\frac{p(y)}{q(y)}=C$
\end_inset

, for some 
\begin_inset Formula $C$
\end_inset

, which implies that 
\begin_inset Formula $p(y)=e^{C}q(y)$
\end_inset

.
 Since 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 both integrate to one, 
\begin_inset Formula $C=0$
\end_inset

 and so 
\begin_inset Formula $p=q$
\end_inset

 -- a contradiction.
\end_layout

\begin_layout Subsection
Wild Bootstrap Testing
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sub:details_testing"

\end_inset


\end_layout

\begin_layout Standard

\lang english
The two concepts required to derive the distribution of the test statistic
 are: 
\begin_inset Formula $\tau$
\end_inset

-mixing 
\begin_inset CommandInset citation
LatexCommand citep
key "dedecker2007weak,leucht_dependent_2013"

\end_inset

, and V-statistics 
\begin_inset CommandInset citation
LatexCommand citet
key "serfling80"

\end_inset

.
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Formula $\tau$
\end_inset

-mixing is a notion of dependence within the observations, weak enough for
 most practical applications.
 Trivially, i.i.d.
 observations are 
\begin_inset Formula $\tau$
\end_inset

-mixing.
 As for Markov chains, whose convergence we study in the experiments, the
 property of geometric ergodicity implies 
\begin_inset Formula $\tau$
\end_inset

-mixing (given that the stationary distribution has a finite moment of some
 order: see Appendix B of 
\begin_inset CommandInset citation
LatexCommand citet
key "chwialkowski2014kernel"

\end_inset

).
 For further details on 
\begin_inset Formula $\tau$
\end_inset

-mixing, see 
\begin_inset CommandInset citation
LatexCommand citet
key "dedecker2005new,dedecker2007weak"

\end_inset

.
 For this work we will assume a technical condition 
\begin_inset Formula $\sum_{t=1}^{\infty}t^{2}\sqrt{\tau(t)}\leq\infty$
\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\lang english
To formally introduce 
\begin_inset Formula $\tau$
\end_inset

-mixing, let 
\begin_inset Formula $\{Z_{t},\mathcal{G}_{t}\}_{t\in\mathbb{N}}$
\end_inset

 be a stationary sequence of integrable random variables with respect to
 a natural filtration 
\begin_inset Formula $\mathcal{G}_{t}$
\end_inset

.
 The sequence is called 
\begin_inset Formula $\tau$
\end_inset

-dependent if 
\begin_inset Formula $\tau(r)$
\end_inset

 converges to zero with 
\begin_inset Formula $r$
\end_inset

 going to infinity, where 
\begin_inset Formula $\tau$
\end_inset

 is defined as follows 
\begin_inset Formula 
\begin{align*}
\tau(r) & :=\sup_{l\in\mathbb{N}}\frac{1}{l}\sup_{r\leq i_{1}...\leq i_{l}}L(Z_{i_{1}},...,Z_{i_{l}})\quad\text{with}\\
L(Z) & :=\ev\left(\sup_{g\in\Lambda}\left|\int g(t)(dP_{Z|\mathcal{\mathcal{G}}_{0}}-dP)\right|\right),
\end{align*}

\end_inset

where 
\begin_inset Formula $\Lambda$
\end_inset

 is the set of all one-Lipschitz continuous real-valued functions on the
 domain of 
\begin_inset Formula $Z$
\end_inset

.
 
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard

\lang english
A direct application of Theorem 2.1 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

 characterizes the limiting behavior of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $nV_{n}$
\end_inset

 for 
\begin_inset Formula $\tau$
\end_inset

-mixing processes,
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\begin_layout Proposition

\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm: null_dist"

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
Under the null hypothesis 
\begin_inset Formula $nV_{n}$
\end_inset

 converges weakly to some distribution.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
The proof, which is a simple verification of the assumptions, can be found
 in the Appendix.
 Although a formula for a limit distribution of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 can be derived explicitly (
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
Theorem 2.1
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

), we do not provide it here.
 To our knowledge there are no methods of obtaining quantiles of a limit
 of 
\family default
\series bold
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $V_{n}$
\end_inset


\family roman
\series medium
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 in closed form.
 The common solution is to estimate quantiles by a resampling method, as
 described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-Kernel-Goodness-of-fit"

\end_inset

.
 The validity of this resampling method is guaranteed by the following propositi
on (which follows from Theorem 2.1 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

 and modification of the Lemma 5 
\begin_inset CommandInset citation
LatexCommand citet
key "chwialkowski2014wild"

\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 ) , proved in the supplement.
\end_layout

\begin_layout Proposition

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
\lang english
\begin_inset CommandInset label
LatexCommand label
name "thm:wild_bootstrap_works"

\end_inset

Let 
\begin_inset Formula $f(W_{1,n},\cdots,W_{t,n})=\sup_{x}|P(nB_{n}>x|W_{1,n},\cdots,W_{t,n})-P(nV_{n}>x)|$
\end_inset

 be a difference between quantiles.
 Under the null hypothesis, 
\begin_inset Formula $f(W_{1,n},\cdots,W_{t,n})$
\end_inset

 converges to zero in probability.
 Under the alternative hypothesis, 
\begin_inset Formula $B_{n}$
\end_inset

 converges to zero, while 
\begin_inset Formula $V_{n}$
\end_inset

 converges to a positive constant.
\end_layout

\begin_layout Standard
As a consequence, if the null hypothesis is true, we can approximate any
 quantile; while under the alternative hypothesis, all quantiles of 
\begin_inset Formula $B_{n}$
\end_inset

 collapse to zero while 
\begin_inset Formula $P(V_{n}>0)\to1$
\end_inset

.
\end_layout

\begin_layout Section

\lang english
Experiments
\end_layout

\begin_layout Standard

\lang english
\begin_inset CommandInset label
LatexCommand label
name "sec:experiment"

\end_inset


\end_layout

\begin_layout Standard

\lang english
We provide a number of experimental applications for our test.
 We begin with a simple check to establish correct test calibration on non-i.i.d.
 data, followed by a demonstration of statistical model criticism for Gaussian
 Process (GP) regression.
 We then apply the proposed test to quantify bias-variance trade-offs in
 MCMC, and demonstrate how to use the test to verify whether MCMC samples
 are drawn from the desired stationary distribution.
 In the final experiment, we move away from the MCMC setting, and
\lang british
 use the test to evaluate the convergence of a nonparametric density estimator.
\end_layout

\begin_layout Subsubsection*

\lang english
Student's t vs Normal
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset Graphics
	filename img/sgld_student_bad.pdf

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Large autocovariance, unsuitable bootstrap.
 The parameter 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}$
\end_inset

 is too large and the bootstrapped V-statistics 
\begin_inset Formula $B_{n}$
\end_inset

 are, on average, too low.
 Therefore it is very likely that 
\begin_inset Formula $V_{n}>B_{n}$
\end_inset

 and the test is too conservative.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
\lang british

\begin_inset CommandInset label
LatexCommand label
name "fig:student_bad"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
In our first task, we modify experiment 4.1 from 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

.
 The null hypothesis is that the observed samples come from a standard normal
 distribution.
 We study the power of the test against samples from a Student's t distribution.
 We expect to observe low p-values when testing against a Student's t distributi
on with few degrees of freedom.
 We considered 1, 5, 10 or 
\begin_inset Formula $\infty$
\end_inset

 degrees of freedom, where 
\begin_inset Formula $\infty$
\end_inset

 is equivalent to sampling from a standard normal distribution.
 For a fixed number of degrees of freedom we drew 1400 samples and calculated
 the p-value.
 This procedure was repeated one hundred times, and the bar plots of p-values
 are shown in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

,
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

.
 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset Graphics
	filename img/sgld_student.pdf

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Large autocovariance, suitable bootstrap.
 The parameter 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}$
\end_inset

is chosen suitably, but due to a large autocorrelation withing the samples,
 the power of the test is small (effective sample size is small).
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset CommandInset label
LatexCommand label
name "fig:studentst"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The twist on 
\lang english
the original experiment 4.1 by 
\begin_inset CommandInset citation
LatexCommand citealt
key "gorham2015measuring"

\end_inset

 is that in our case, the draws from the Student's t distribution were given
 temporal correlation.
 The samples were generated using a Metropolis–Hastings algorithm, with
 a Gaussian random walk (variance equal to 0.5).
 We emphasize the need for an appropriate choice of the wild bootstrap process
 parameter, 
\begin_inset Formula $a_{n}$
\end_inset

, which indicates the probability of a sign flip.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:student_bad"

\end_inset

 we plot p-values for 
\begin_inset Formula $a_{n}$
\end_inset

 being set to 
\begin_inset Formula $0.5$
\end_inset

.
 Such a high value of 
\begin_inset Formula $a_{n}$
\end_inset

 is 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
suitable for iid observations, but results in p-values that are too conservative
 for temporally correlated observations.
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:studentst"

\end_inset

, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $a_{n}=0.02$
\end_inset

, which gives a well calibrated distribution of the p-values under the null
 hypothesis (see box plot for an infinite number degrees of freedom), however
 the power of the test is reduced.
 Indeed, p-values for five degrees of freedom are already large.
 The solution that we recommend is a mixture of thinning and adjusting 
\begin_inset Formula $a_{n},$
\end_inset

 as presented in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:thinning"

\end_inset

.
 We have thinned the observations by a factor of 20 and set 
\begin_inset Formula $a_{n}=0.1$
\end_inset

, thus preserving both good statistical power and correct calibration of
 p-values under the null hypothesis.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/sgld_student_opt.pdf

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Thinned sample, suitable bootstrap.
 Most of the 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
auto-correlation within the sample is canceled by thinning.
 To guarantee that the remaining autocorrelation is handled properly, the
 flip probability is set at 
\begin_inset Formula $0.1$
\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\lang british

\begin_inset CommandInset label
LatexCommand label
name "fig:thinning"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Statistical Model Criticism on Gaussian Processes
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/gp_regression_data_fit.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Fitted GP and data used to fit (blue) and to apply test (red).
\begin_inset CommandInset label
LatexCommand label
name "fig:experiment_gp_fit"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We next apply our test to the problem of statistical model criticism for
 GP regression.
 Our presentation and approach are similar to the non i.i.d.
 case of 
\begin_inset CommandInset citation
LatexCommand citet
after "Section 6"
key "lloyd2015statistical"

\end_inset

.
 We used the Solar dataset, consisting of a 1D regression problem with 
\begin_inset Formula $N=402$
\end_inset

 pairs 
\begin_inset Formula $(X,y)$
\end_inset

.
 We fit 
\begin_inset Formula $N_{\text{train}}=361$
\end_inset

 data using a GP with a squared exponential kernel and a Gaussian noise
 model, and performed standard maximum likelihood II on the hyperparameters
 (length-scale, overall scale, noise-variance).
 We then applied our test to the remaining 
\begin_inset Formula $N_{\text{test}}=41$
\end_inset

 data.
 Our test attempts to falsify the null hypothesis that the Solar dataset
 was generated from the predictive distribution (conditioned on training
 data and predicted position) of the GP.
 
\begin_inset CommandInset citation
LatexCommand citet
key "lloyd2015statistical"

\end_inset

 refer to this setup as non i.i.d., since the predictive distribution is a
 different univariate Gaussian for every predicted point.
 Note that in contrast to their MMD-based method, our test does 
\emph on
not
\emph default
 need to simulate from the multiple predictive distributions.
 Our particular 
\begin_inset Formula $N_{\text{train}},N_{\text{test}}$
\end_inset

 were chosen to make sure the GP fit has stabilised, i.e.
 adding more data did not cause further model refinement.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:experiment_gp_fit"

\end_inset

 shows training and testing data, and the fitted GP.
 Clearly, the Gaussian noise model is a poor fit for this particular dataset,
 e.g.
 around 
\begin_inset Formula $X=-1$
\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:experiment_gp_test"

\end_inset

 shows the distribution over 
\begin_inset Formula $D=10000$
\end_inset

 bootstrapped V-statistics 
\begin_inset Formula $B_{n}$
\end_inset

 with 
\begin_inset Formula $n=N_{\text{test}}$
\end_inset

.
 The 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
bootstrapped 
\begin_inset Formula $B_{n}$
\end_inset

 are not distributed according to the true null distribution (which is only
 guaranteed when the null holds), however
\end_layout

\end_inset

 test statistic lies in an upper quantile of the bootstrapped null distribution,
 indicating (correctly) that it is unlikely the test points were generated
 by the fitted GP model, even for the low number of test data observed,
 
\begin_inset Formula $N_{\text{test}}=41$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/gp_regression_bootstrap_hist.eps
	scale 85

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Bootstrapped 
\begin_inset Formula $B_{n}$
\end_inset

 distribution with the test statistic 
\begin_inset Formula $V_{n}$
\end_inset

 marked.
\begin_inset CommandInset label
LatexCommand label
name "fig:experiment_gp_test"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*

\lang english
Approximate MCMC algorithm 
\end_layout

\begin_layout Standard

\lang english
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset Graphics
	filename img/Heiko1.pdf

\end_inset


\lang british

\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Distribution of p-values as a function of 
\begin_inset Formula $\epsilon$
\end_inset

 for austerity MCMC.
 
\begin_inset CommandInset label
LatexCommand label
name "p-values"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\lang english
We show how to quantify
\lang british
 bias-variance trade-offs in an approximate 
\lang english
MCMC algorithm
\lang british
 -- 
\lang english
austerity MCMC 
\begin_inset CommandInset citation
LatexCommand citep
key "korattikara2013austerity"

\end_inset


\lang british
.
 
\lang english
For the purpose of illustration we use a simple generative model from 
\begin_inset CommandInset citation
LatexCommand citet
key "gorham2015measuring,welling2011bayesian"

\end_inset

, 
\begin_inset Formula 
\begin{align*}
\theta_{1}\sim N(0,10);\theta_{2}\sim N(0,1)\\
X_{i}\sim\frac{1}{2}N(\theta_{1},4)+\frac{1}{2}N(\theta_{2},4) & .
\end{align*}

\end_inset


\lang british

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\lang english
\begin_inset Graphics
	filename img/Heiko2.pdf

\end_inset


\lang british

\begin_inset Caption Standard

\begin_layout Plain Layout

\lang english
Average number of likelihood evaluations a function of 
\begin_inset Formula $\epsilon$
\end_inset

 for austerity MCMC
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
(the y-axis is in millions of evaluations)
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 
\begin_inset CommandInset label
LatexCommand label
name "lik-evals"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\lang english
Austerity MCMC is a Monte Carlo procedure designed to reduce the number
 of likelihood evaluation in the acceptance step of the Metropolis-Hastings
 algorithm.
 The crux of method is to look at only a subset of the data, and make an
 acceptance/rejection decision based on this subset.
 The probability of making a wrong decision is proportional to a parameter
 
\begin_inset Formula $\epsilon\in[0,1]$
\end_inset

 .
 This parameter influences the time complexity of Austerity MCMC: when 
\begin_inset Formula $\epsilon$
\end_inset

 is larger, i.e., when there is a greater tolerance for error, the expected
 computational cost is lower.
 We simulated 
\begin_inset Formula $\{X_{i}\}_{1\leq i\leq400}$
\end_inset

 points from the model with 
\begin_inset Formula $\theta_{1}=0$
\end_inset

 and 
\begin_inset Formula $\theta_{2}=1$
\end_inset

.
 In this setting there were two modes in the posterior distribution: one
 at 
\begin_inset Formula $(0,1)$
\end_inset

 and the other at 
\begin_inset Formula $(1,-1)$
\end_inset

.
 We ran the Austerity algorithm with 
\begin_inset Formula $\epsilon$
\end_inset

 varying over the range 
\begin_inset Formula $[0.001,0.2]$
\end_inset

.
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we calculated an individual thinning factor, such that correlation between
 consecutive 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\lang english
elements of
\end_layout

\end_inset

 samples from the chains was smaller than 
\begin_inset Formula $0.5$
\end_inset

 (greater 
\begin_inset Formula $\epsilon$
\end_inset

 generally required more thinning).
 For each 
\begin_inset Formula $\epsilon$
\end_inset

 we tested the hypothesis that 
\begin_inset Formula $\{\theta_{i}\}_{1\leq i\leq500}$
\end_inset

 were drawn from the true stationary posterior, using our goodness of fit
 test.
 We generated 100 p-values for each 
\begin_inset Formula $\epsilon$
\end_inset

 , as shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "p-values"

\end_inset

.
 It is clear that 
\begin_inset Formula $\epsilon=0.09$
\end_inset

 yields a good approximation of the true stationary distribution, while
 being parsimonious in terms of likelihood evaluations, as shown in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "lik-evals"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection*
Convergence in non-parametric density estimation
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_data_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Density estimation: P-values for an increasing number of data 
\begin_inset Formula $N$
\end_inset

 for the non-parametric model.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_data"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In our final experiment, we apply our goodness of fit test to measuring
 quality-of-fit in nonparametric density estimation.
 We evaluate two density models: the infinite dimensional exponential family
 
\begin_inset CommandInset citation
LatexCommand citep
key "SriFukKumGreHyv14"

\end_inset

, and a recent approximation to this model using random Fourier features
 
\begin_inset CommandInset citation
LatexCommand citep
key "strathmann2015gradient"

\end_inset

.
 Our implementation of the model assumes the log density to take the form
 
\begin_inset Formula $f(x)$
\end_inset

, where 
\begin_inset Formula $f$
\end_inset

 lies in an RKHS induced by a Gaussian kernel with bandwidth 
\begin_inset Formula $1$
\end_inset

.
 We fit the model using 
\begin_inset Formula $N$
\end_inset

 observations drawn from a standard Gaussian, and performed our quadratic
 time test on a separate evaluation dataset of fixed size, 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

.
 Our goal was to identify 
\begin_inset Formula $N$
\end_inset

 sufficiently large that the goodness of fit test did not reject the null
 hypothesis (i.e., the model had learned the density sufficiently well, bearning
 in mind that it is guaranteed to converge for sufficiently large 
\begin_inset Formula $N$
\end_inset

).
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_data"

\end_inset

 shows how the distribution of p-values evolves as a function of 
\begin_inset Formula $N$
\end_inset

; this distribution is uniform for 
\begin_inset Formula $N=5000$
\end_inset

, but at 
\begin_inset Formula $N=500$
\end_inset

, the null hypothesis would very rarely be rejected.
\end_layout

\begin_layout Standard
We next consider the random fourier feature approximation to this model,
 where the log pdf, 
\begin_inset Formula $f$
\end_inset

, is approximated using a finite dictionary of random Fourier features 
\begin_inset CommandInset citation
LatexCommand citep
key "Rahimi2007"

\end_inset

.
 The natural question when using this approximation is: 
\begin_inset Quotes eld
\end_inset

How many random features do it I need?
\begin_inset Quotes erd
\end_inset

 Using the same test power 
\begin_inset Formula $N_{\text{test}}=500$
\end_inset

 as above, and a large number of available samples 
\begin_inset Formula $N=5\cdot10^{4}$
\end_inset

, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:density_estimation_increasing_features"

\end_inset

 shows the distributions of p-values for an increasing number of random
 features 
\begin_inset Formula $m$
\end_inset

.
 From about 
\begin_inset Formula $m=50$
\end_inset

, the null hypothesis would rarely be rejected, given a reasonable choice
 of test level.
 Note, however, that the p-values do 
\emph on
not
\emph default
 have a uniform distribution, even for a large number of random features.
 This subtle effect is caused by over-smoothing due to the regularisation
 approach taken in 
\begin_inset CommandInset citation
LatexCommand citep
after "KMC finite"
key "strathmann2015gradient"

\end_inset

, which would not otherwise have been detected.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
It vanishes when the estimator is not regularised, however, at the cost
 of numerical instability.
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/increasing_features_fixed_test.eps

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Approximate density estimation: P-values for an increasing number of random
 features 
\begin_inset Formula $m$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:density_estimation_increasing_features"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
FloatBarrier
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "biblio"
options "icml2015"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
normalsize
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
onecolumn
\end_layout

\end_inset


\end_layout

\begin_layout Part*
Appendix
\end_layout

\begin_layout Section

\lang english
Proofs
\end_layout

\begin_layout Lemma

\lang english
\begin_inset CommandInset label
LatexCommand label
name "lem:easy"

\end_inset

 If a random variable 
\begin_inset Formula $X$
\end_inset

 is distributed according to 
\begin_inset Formula $p$
\end_inset

, then for all 
\begin_inset Formula $f\in\mathcal{F}$
\end_inset

, the expected value of 
\begin_inset Formula $T$
\end_inset

 is zero, i.e.
 
\begin_inset Formula $\ev(Tf)(X)=0$
\end_inset

.
\end_layout

\begin_layout Proof

\lang english
This result was proved on bounded domains 
\begin_inset Formula $\mathcal{X}\subset\mathbb{R}^{d}$
\end_inset

 by 
\begin_inset CommandInset citation
LatexCommand citet
after "Lemma 1"
key "OatGirCho15"

\end_inset

, under conditions on the kernel
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\lang english
%AG: added discussion of Oates' proof.
\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
0 & =\oint_{\partial\mathcal{X}}k(x,x')p(x)n(x)dS(x'),\\
0 & =\oint_{\partial\mathcal{X}}\nabla_{x}k(x,x')^{\top}n(x')p(x')dS(x'),
\end{align*}

\end_inset

where 
\begin_inset Formula $n(x)$
\end_inset

 is the unit vector normal to the boundary at 
\begin_inset Formula $x$
\end_inset

, and 
\begin_inset Formula $\oint_{\partial\mathcal{X}}$
\end_inset

 is the surface integral over the boundary 
\begin_inset Formula $\partial\mathcal{X}$
\end_inset

.
 The case of unbounded domains was discussed by 
\begin_inset CommandInset citation
LatexCommand citet
after "Remark 2"
key "OatGirCho15"

\end_inset

.
 Here we provide an alternative proof for the latter case, with simpler
 conditions.
 First we show that the functions 
\begin_inset Formula $g_{i}=p\cdot f_{i}$
\end_inset

 vanish at infinity, by which we mean that for all dimensions 
\begin_inset Formula $j$
\end_inset

 
\begin_inset Formula 
\[
\lim_{x_{j}\to\infty}g_{i}(x_{1},\cdots,x_{d})=0.
\]

\end_inset

The density function 
\begin_inset Formula $p$
\end_inset

 vanishes at infinity.
 The function 
\begin_inset Formula $f$
\end_inset

 is bounded, which is implied by Cauchy-Schwarz inequality -- 
\begin_inset Formula $\left|f(x)\right|\le\left\Vert f\right\Vert \sqrt{k(x,x)}$
\end_inset

.
 This implies that the function 
\begin_inset Formula $g$
\end_inset

 vanishes at infinity.
 To show the expected value 
\begin_inset Formula $\ev(T_{p})f(X)$
\end_inset

 is zero, it is sufficient to show that for all dimensions 
\begin_inset Formula $i$
\end_inset

, the expected value of 
\begin_inset Formula $\frac{\partial\log p(X)}{\partial x_{i}}f_{i}(X)+\frac{\partial f_{i}(X)}{\partial x_{i}}$
\end_inset

 is zero.
 
\begin_inset Formula 
\begin{align*}
 & \ev\left(\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right)\\
 & =\int_{R_{d}}\left[\frac{\partial\log p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{1}{p(x)}\frac{\partial q(x)}{\partial x_{i}}f(x)+\frac{\partial f(x)}{\partial x_{i}}\right]q(x)dx\\
 & =\int_{R_{d}}\left[\frac{\partial p(x)}{\partial x_{i}}f_{i}(x)+\frac{\partial f_{i}(x)}{\partial x_{i}}q(x)\right]dx\\
 & \overset{(a)}{=}\int_{R_{d-1}}\left(\lim_{R\to\infty}p(x)f_{i}(x)\bigg|_{x_{i}=-R}^{x_{i}=R}\right)dx_{1}\cdots dx_{i-1}\cdots dx_{i+1}\cdots d{x_{d}}\\
 & =\int_{R_{d-1}}0dx_{1}\cdots dx_{i-1}\cdots dx_{i+1}\cdots d{x_{d}}\\
 & =0.
\end{align*}

\end_inset

For the equation (a) we have used integration by parts, fact that 
\begin_inset Formula $g_{i}$
\end_inset

 vanishes at infinity and Fubini-Toneli theorem to show that we can do iterated
 integration.
 The sufficient condition for the Fubini-Toneli theorem is that 
\begin_inset Formula $\int|g_{i}f(x)|q(x)dx$
\end_inset

 exists.
 This is implied by existence of 
\begin_inset Formula $\ev|\frac{\partial\log p(X)}{\partial x_{i}}f_{i}(X)|$
\end_inset

 and 
\begin_inset Formula $\ev|\frac{\partial f_{i}(X)}{\partial x_{i}}|$
\end_inset

.
 Since 
\begin_inset Formula $f_{i}$
\end_inset

 is bounded and 
\begin_inset Formula $\ev|\frac{\partial\log p(X)}{\partial x_{i}}|\leq\ev\|\nabla\log p(Z)\|^{2}$
\end_inset

, condition ii) guarantees that 
\begin_inset Formula $\ev|\frac{\partial\log p(X)}{\partial x_{i}}f_{i}(X)|$
\end_inset

 is finite.
 For the second term we have 
\begin_inset Formula $\ev|\frac{\partial f_{i}(X)}{\partial x_{i}}|=\ev|\langle\frac{\partial k(X,cdot)}{\partial x_{i}},f_{i}|\rangle\leq\|f_{i}\|_{\mathcal{F}}\ev\sqrt{\frac{\partial^{2}k(X,X)}{dx_{i}dx_{i+d}}}$
\end_inset

, which is guaranteed by the condition iv).
 
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of 
\lang british
proposition
\lang english
 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset


\end_layout

\end_inset

We check assumptions of the Theorem 2.1 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

.
 The condition A1, 
\begin_inset Formula $\sum_{t=1}^{\infty}\sqrt{\tau(t)}\leq\infty$
\end_inset

, is implied by assumption 
\begin_inset Formula $\sum_{t=1}^{\infty}t^{2}\sqrt{\tau(t)}\leq\infty$
\end_inset

 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

.
 Condition A2 (iv), Lipschitz continuity of 
\begin_inset Formula $h$
\end_inset

, follows from assumption (iv).
 Conditions A2 i), ii) positive definiteness, symmetry and degeneracy of
 
\begin_inset Formula $h$
\end_inset

 follow from the proof of Theorem 
\begin_inset CommandInset ref
LatexCommand eqref
reference "theorem_discrepancy_is_metric"

\end_inset

.
 Indeed
\end_layout

\begin_layout Proof

\lang english
\begin_inset Formula 
\[
h(x,y)=\langle\left[\nabla\log p(x)k(x,\cdot)+\nabla_{1}k(x,\cdot)\right],\left[\nabla\log p(y)k(y,\cdot)+\nabla_{1}k(y,\cdot)\right]\rangle_{\mathcal{F}^{d}}
\]

\end_inset


\begin_inset Newline newline
\end_inset

so the statistic is an inner product and hence positive definite.
 Degeneracy under the null follows from the fact that for any 
\begin_inset Formula $t$
\end_inset

, by Lemma 
\begin_inset CommandInset ref
LatexCommand eqref
reference "lem:easy"

\end_inset

,
\begin_inset Formula $\ev(\nabla\log p(x)k(x,t)+\nabla_{1}k(x,t))=0$
\end_inset

.
 Finally, condition A2 (iii), 
\begin_inset Formula $\ev h(X,X)\leq\infty$
\end_inset

 follows from assumptions (ii), (iii) and boundedness of the kernel.
\end_layout

\begin_layout --Separator--

\end_layout

\begin_layout Proof

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Proof of 
\lang british
proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:wild_bootstrap_works"

\end_inset


\end_layout

\end_inset

We use Theorem 2.1 
\begin_inset CommandInset citation
LatexCommand citep
key "leucht2012degenerate"

\end_inset

 to see that, 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
under the null hypothesis, 
\begin_inset Formula $f(W_{1,n},\cdots,W_{t,n})$
\end_inset

 converges to zero in probability.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
 We have checked assumptions A1,A2 in the proof of the proposition 
\lang british

\begin_inset CommandInset ref
LatexCommand ref
reference "thm: null_dist"

\end_inset


\lang english
.
 Assumption B1 is identical to our assumption from Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Details"

\end_inset

 .
 Finally we check assumption B2 (bootstrap assumption):
\emph on
 
\begin_inset Formula $\{W_{t,n}\}_{1\leq t\leq n}$
\end_inset


\emph default
 is a row-wise strictly stationary triangular array independent of all 
\begin_inset Formula $Z_{t}$
\end_inset

 such that 
\begin_inset Formula $\ev W_{t,n}=0$
\end_inset

 and 
\begin_inset Formula $\sup_{n}\ev|W_{t,n}^{2+\sigma}|=1<\infty$
\end_inset

 for some 
\begin_inset Formula $\sigma>0$
\end_inset

.
 The auto-covariance of the process is given by 
\begin_inset Formula $\ev W_{s,n}W_{t,n}=(1-2p_{n})^{-|s-t|}$
\end_inset

, so the function 
\begin_inset Formula $\rho(x)=\exp(-x)$
\end_inset

, and 
\begin_inset Formula $l_{n}=\log(1-2p_{n})^{-1}$
\end_inset

.
 We verify that 
\begin_inset Formula $\lim_{u\to0}\rho(u)=1$
\end_inset

.
 If we set 
\begin_inset Formula $p_{n}=w_{n}^{-1}$
\end_inset

 , such that 
\begin_inset Formula $w_{n}=o(n)$
\end_inset

 and 
\begin_inset Formula $\lim_{n\to\infty}w_{n}=\infty$
\end_inset

, then 
\begin_inset Formula $l_{n}=O(w_{n})$
\end_inset

 and 
\begin_inset Formula $\sum_{r=1}^{n-1}\rho(|r|/l_{n})=\frac{1-(1-2p_{n})^{n+1}}{p_{n}}=O(w_{n})=O(l_{n})$
\end_inset

.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\lang english
, what we had show
\end_layout

\end_inset

 Finally we show that 
\begin_inset Formula $B_{n}$
\end_inset

 converges to zero under the alternative.
 It is sufficient to check that 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\ev B_{n}\to0,\ev B_{n}^{2}\to0.$
\end_inset


\end_layout

\begin_layout Proof

\lang english
\begin_inset Formula 
\begin{align*}
\ev B_{n} & =\frac{1}{n^{2}}\sum_{i,j}\ev W_{i}W_{j}\ev h(Z_{i},Z_{j})\\
 & =\frac{1}{n^{2}}\sum_{i\in N^{m}}\rho(|j-i|/l_{n})\ev h(Z_{j},Z_{i})\\
 & \leq\frac{1}{n^{2}}\sum_{i\in N^{m}}\rho(|j-i|/l_{n})C\\
 & \to0
\end{align*}

\end_inset


\lang british
for some constant 
\begin_inset Formula $C=\ev h(Z_{1},Z_{2})$
\end_inset

, whose existence follows from assumptions i) and iii).
 As for 
\begin_inset Formula $\ev B_{n}^{2}$
\end_inset

, we have
\end_layout

\begin_layout Proof

\lang english
\begin_inset Formula 
\begin{align*}
\ev B_{n}^{2} & =\frac{1}{n^{4}}\sum_{i,j,k,l}\ev W_{i}W_{j}W_{k}W_{l}\ev h(Z_{i},Z_{j})h(Z_{k},Z_{l})\\
 & \leq\frac{1}{n^{4}}\sum_{i\neq j,i\neq k,i\neq l,j\neq k,j\neq l,k\neq l}\ev W_{i}W_{j}W_{k}W_{l}\ev h(Z_{i},Z_{j})^{2}\ev h(Z_{k},Z_{l})^{2}+C'\frac{6n^{3}}{n^{4}}\\
 & \leq\frac{1}{n^{4}}\sum_{i\neq j,i\neq k,i\neq l,j\neq k,j\neq l,k\neq l}\ev W_{i}W_{j}W_{k}W_{l}C'+\frac{6C'}{n}\\
 & =\frac{6C'}{n}\to0,
\end{align*}

\end_inset


\lang british
where 
\begin_inset Formula $C'=\ev h(Z_{i},Z_{j})^{2}\ev h(Z_{k},Z_{l})^{2}$
\end_inset

.
\end_layout

\begin_layout Subsection

\lang english
Linear time test 
\end_layout

\begin_layout Standard

\lang english
We may use similar reasoning for the quadratic time test to define a linear
 time test, based on the two-sample test of 
\begin_inset CommandInset citation
LatexCommand citet
key "Chwialkowski2015"

\end_inset

.
 For some fixed location 
\begin_inset Formula $y$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

, define a random variable 
\begin_inset Formula $s(X,y)$
\end_inset

 as 
\begin_inset Formula 
\begin{align}
s(X,y)=\nabla\log p(X)g(X,y)-\nabla g(X,y).
\end{align}

\end_inset

For some number of random locations 
\begin_inset Formula $Y_{1},Y_{J}$
\end_inset

 and a random variable 
\begin_inset Formula $X$
\end_inset

 define a random vector 
\begin_inset Formula $Z_{i}$
\end_inset

 
\begin_inset Formula 
\begin{equation}
Z_{i}=(s(X_{i},Y_{1}),\cdots,s(X_{i},Y_{J}))\in\mathbf{R}^{J}.
\end{equation}

\end_inset

Let 
\begin_inset Formula $W_{n}$
\end_inset

 be a mean of 
\begin_inset Formula $Z_{i}$
\end_inset

's 
\begin_inset Formula $W_{n}=\frac{1}{n}\sum_{i=1}^{n}Z_{i},$
\end_inset

 and 
\begin_inset Formula $\Sigma_{n}$
\end_inset

 its covariance matrix 
\begin_inset Formula $\Sigma_{n}=\frac{1}{n}ZZ^{T}$
\end_inset

.
 The test statistic is 
\begin_inset Formula 
\begin{equation}
S_{n}=nW_{n}\Sigma_{n}^{-1}W_{n}.
\end{equation}

\end_inset

The computation of 
\begin_inset Formula $S_{n}$
\end_inset

 requires inversion of a 
\begin_inset Formula $J\times J$
\end_inset

 matrix 
\begin_inset Formula $\Sigma_{n}$
\end_inset

, but this is fast and numerically stable: 
\begin_inset Formula $J$
\end_inset

 will typically be small, and is less than 10 in our experiments.
 The next proposition demonstrates the use of 
\begin_inset Formula $S_{n}$
\end_inset

 as a one-sample test.
\end_layout

\begin_layout Proposition

\lang english
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\lang english
Asymptotic behavior of 
\begin_inset Formula $S_{n}$
\end_inset


\end_layout

\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "prop:Hotelling"

\end_inset

 If 
\begin_inset Formula $\ev s(X,y)=0$
\end_inset

 for all 
\begin_inset Formula $y$
\end_inset

, then the statistic 
\begin_inset Formula $S_{n}$
\end_inset

 is a.s.
 asymptotically distributed as a 
\begin_inset Formula $\chi^{2}$
\end_inset

-random variable with 
\begin_inset Formula $Jd$
\end_inset

 degrees of freedom, where 
\begin_inset Formula $d$
\end_inset

 is 
\begin_inset Formula $X$
\end_inset

 dimensionality (as 
\begin_inset Formula $n\to\infty$
\end_inset

 with 
\begin_inset Formula $d$
\end_inset

 fixed).
 If 
\begin_inset Formula $\ev s(X,y)\neq0$
\end_inset

 for almost all 
\begin_inset Formula $y$
\end_inset

 then a.s.
 for any fixed 
\begin_inset Formula $r$
\end_inset

, 
\begin_inset Formula $\mathbb{P}(S_{n}>r)\to1$
\end_inset

 as 
\begin_inset Formula $n\to\infty$
\end_inset

 .
 
\end_layout

\begin_layout Paragraph

\lang english
One sample test
\end_layout

\begin_layout Standard

\lang english
Calculate 
\begin_inset Formula $S_{n}$
\end_inset

.
 Choose a threshold 
\begin_inset Formula $r_{\alpha}$
\end_inset

 corresponding to the 
\begin_inset Formula $1-\alpha$
\end_inset

 quantile of a 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution with 
\begin_inset Formula $J$
\end_inset

 degrees of freedom, and reject the null hypothesis whenever 
\begin_inset Formula $S_{n}$
\end_inset

 is larger than 
\begin_inset Formula $r_{\alpha}$
\end_inset

.
 
\end_layout

\end_body
\end_document
