\documentclass{beamer}
\usepackage{movie15}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsthm,amsthm}
\usepackage{amssymb}
\global\long\def\ev{\mathbb{E}}
\usetheme{metropolis}           % Use metropolis theme
\title{ A Kernel Test of Goodness of Fit}
\date{\today}
\author{Kacper Chwialkowski$^*$, Heiko Strathmann$^*$, Arthur Gretton}
\institute{}
\titlegraphic{
    %\includegraphics[width=2cm]{csml_logo_vector2.pdf}\hspace*{4.75cm}~%
   \includegraphics[width=2cm]{./img/csml_logo_vector2.pdf}
}


\newtheorem{thm}{Theorem}



\begin{document}
\frame{\titlepage}
  \setbeamercolor{background canvas}{bg=white}
 \begin{frame}{What is one sample testing?}
 \begin{center}
$\theta_{1}\sim{\cal N}(0,10);\theta_{2}\sim{\cal N}(0,1)$\\
$ X_{i}\sim\frac{1}{2}{\cal N}(\theta_{1},4)+\frac{1}{2}{\cal N}(\theta_{1}+\theta_{2},4) $.
 \includegraphics[width=0.6\textwidth]{./img/sgld_sample_wth_trace.pdf} 
 \end{center}
 \end{frame}
 
 \begin{frame}{Maximum mean discrepancy}
 \begin{center}
$MMD(p,q,F) = \sup_{f \in F} [\ev_{q}f - \ev_{p} f]  $\\
\vspace{0.5cm}
 \includegraphics[width=0.6\textwidth]{./img/mmd.pdf} 
 \end{center}
 \end{frame} 
 
 
 
 \begin{frame}{Main idea (by Stein)}
To get rid of $\ev_{p}f$  in $$ \sup_{f \in F} [\ev_{q}f - \ev_{p} f ] $$we will use the cornerstone of modern ML

\pause
\textbf{integration by parts}

\pause 
not the chain rule

\pause
I got you

\pause 
\begin{flushright}
\small \textit{Kacper} 
\end{flushright}



\end{frame} 

  \begin{frame}{Cornerstone of modern ML: integration by parts}
  \begin{center}
  Consider the  class  \scalebox{1.3}{ $G = \{ f  +  \log' q \cdot  f | f \in F \}$} justified by 
\begin{align*}
 0= &  f(x) p(x)  \big|_{x=-\infty}^{x=\infty} \\
   = &  \int_{-\infty}^{\infty} (f(x) p(x) )'  dx \\
   = &  \int   f(x)' q(x)   + f(x)p'(x)  \\
   = &  \ev_p f(X)  +  \log' p(X) f(X) \\
   = & \ev_p g(X), \\
    & \quad \quad \quad  \quad  \text{ where } g \in G
\end{align*}
\end{center}

 \end{frame} 
  

 \begin{frame}{Stein discrepancy }
 \begin{center}
 \scalebox{0.8}{ $G = \{ f  +  \log' p f | f \in F \}$}
 
$MMD(p,q,G) = \sup_{g\in G} \ev_{q}g - \ev_{p} g - = \sup_{g \in G} \ev_{q} g $  \\
\vspace{0.5cm}
 \includegraphics[width=0.6\textwidth]{./img/s1.pdf} 
 \end{center}
 \end{frame} 
  
  
 \begin{frame}{Stein discrepancy}
 \begin{center}
 \scalebox{0.8}{ $G = \{ f  +  \log' p f | f \in F \}$}
 
$MMD(p,q,G) = \sup_{g\in G} \ev_{q}g - \ev_{p} g - = \sup_{g \in G} \ev_{q} g $  \\
\vspace{0.5cm}
 \includegraphics[width=0.6\textwidth]{./img/s05.pdf} 
 \end{center}
 \end{frame} 
 
 
 \begin{frame}{Stein discrepancy}
 \begin{center}
 \scalebox{0.8}{ $G = \{ f  +  \log' p f | f \in F \}$}
 
$MMD(p,q,G) = \sup_{g\in G} \ev_{q}g - \ev_{p} g - = \sup_{g \in G} \ev_{q} g $  \\
\vspace{0.5cm}
 \includegraphics[width=0.6\textwidth]{./img/s01.pdf} 
 \end{center}
 \end{frame} 

 
\begin{frame}{Main results}
\begin{center}
 Let $F$ be the RKHS associated with the kernel $k$. Consider a friendly looking function
\begin{align*}
h_{q}(x,y) & := \partial_{x} \log p(x) \partial_{x} \log p(y) k(x,y)\\
 & \quad+\partial_{y} \log p(y) \partial_{x}  k(x,y)\\
 & \quad+\partial_{x} \log p(x) \partial_{y}k(x,y)\\
 & \quad+\partial_{x} \partial_{y} k(x,y).
\end{align*}
\end{center}
\end{frame}
 
\begin{frame}{Main results}

Let $q,p$ be probability measures and $Z\sim q$. 
\begin{thm}
If $\ev_{q} h_p(Z,Z)<\infty$, then $MMD(p,q,G) = \ev_{q} h_{p}(Z,Z')$.
\end{thm}
\begin{thm}
 If the kernel $k$ is cc-universal, $\ev_{q} h_{q}(Z,Z)<\infty$ and $\ev_{q} (\log' \frac{p(Z)}{q(Z)})^{2}<\infty$
then $MMD(p,q,G) =0$ if and only if $p=q$.
\end{thm}

\end{frame} 


\begin{frame}{Function $h_p$}

 \begin{columns}
        \begin{column}{.5\textwidth}
        \vspace{1cm}
        \begin{figure}
%          \advance\leftskip-3cm
\advance\rightskip-2cm

           \includegraphics[width=\textwidth]{./img/h.pdf} 
        \end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
        \vspace{-2cm}
           \begin{align*}
h_{p}(x,y) & := \partial_{x} \log p(x) \partial_{x} \log p(y) k(x,y)\\
 & \quad+\partial_{y}\log p(y) \partial_{x}  k(x,y)\\
 & \quad+\partial_{x}\log p(x) \partial_{y}k(x,y)\\
 & \quad+\partial_{x} \partial_{y} k(x,y),
\end{align*}
        \end{column}
    \end{columns}
 

 \end{frame}
 
 \begin{frame}{$V$-statistics}
An estimator of $\ev h_p(X,X')$ is
\[
 V_n(h_p) = \frac {1} {n^2} \sum_{i,j=1}^n h_p(X_i,X_j).
\]
Our test statistic is $ n V_n(h_p)$.

If $X_i \sim p$ then $ n V_n(h_p)$  converges weakly. 

Otherwise it does not,  it explodes, $P(n V_n(h_p) <C) \to 0$.
 \end{frame}
 
 
  \begin{frame}{$V$-statistics}
To estimate quantiles of $ V_n(h_p)$  
\[
 V_n(h_p) = \frac {1} {n^2} \sum_{i,j=1}^n h_p(X_i,X_j).
\]
under the null, we use wild bootstrap
\[
 B_n(h_p) = \frac {1} {n^2} \sum_{i,j=1}^n W_i W_j h_p(X_i,X_j).
\]
  where $W_i$ is a specific series of zero valued random variables.
\end{frame}

  \begin{frame}{Student's t vs.~Normal}
\begin{columns}
        \begin{column}{.5\textwidth}
        \begin{figure}
           \includegraphics[width=\textwidth]{img/sgld_student_bad} 
           
           
            \includegraphics[width=\textwidth]{img/sgld_student} 
        \end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
            \begin{figure}
           \includegraphics[width=\textwidth]{img/sgld_student_opt} 
        \end{figure}
       Null -- samples are from normal distribution. Draws from the Student's t distribution exhibit temporal correlation.
        \end{column}
    \end{columns}
 

\end{frame}



  \begin{frame}{Bias quantification in Approximate MCMC}
\begin{columns}
        \begin{column}{.55\textwidth}
        \begin{figure}
           \includegraphics[width=\textwidth]{img/Heiko1} 
           
            \includegraphics[width=\textwidth]{img/Heiko2}
        \end{figure}
        \end{column}
        \begin{column}{.45\textwidth}
        \begin{align*}
\theta_{1}\sim{\cal N}(0,10);\theta_{2}\sim{\cal N}(0,1)\\
X_{i}\sim\frac{1}{2}{\cal N}(\theta_{1},4)+\frac{1}{2}{\cal N}(\theta_{1}+\theta_{2},4) & .
\end{align*}

            \begin{figure}
           \includegraphics[width=\textwidth]{img/sgld_sample} 
        \end{figure}

        \end{column}
    \end{columns}
 \end{frame}

  \begin{frame}{Experiment 3}
\begin{columns}
        \begin{column}{.5\textwidth}
        \begin{figure}
           \includegraphics[width=\textwidth]{img/gp_regression_data_fit} 
             
             \includegraphics[width=\textwidth]{img/gp_regression_bootstrap_hist} 
        \end{figure}
        \end{column}
        \begin{column}{.5\textwidth}
       We use the \texttt{solar} dataset. We fit $N_{\text{train}}=361$
data using a GP, and perform  maximum likelihood. We then apply our test
to the remaining $N_{\text{test}}=41$ data.
        \end{column}
    \end{columns}
 
 \end{frame}


\end{document}